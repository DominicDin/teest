{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DominicDin/teest/blob/main/%E2%80%9Cdzx_Dataset_for_LLM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take Home Test: Reformat a Public Dataset for LLM Training\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal of this task is to prepare public datasets for more effective use in training and fine-tuning Large Language Models (LLMs). You are required to reformat a specific subset of a public dataset into a structured, consistent format to facilitate its usability.\n",
        "\n",
        "### Detailed Instructions\n",
        "\n",
        "#### 1. Dataset Selection and Preparation\n",
        "\n",
        "- **Dataset:** You are assigned the `Headline` subset of the [AdaptLLM/finance-tasks](https://huggingface.co/datasets/AdaptLLM/finance-tasks) dataset.\n",
        "\n",
        "- **Task Description:** Each entry in the `input` column contains multiple \"Yes\" or \"No\" questions alongside their respective answers. Your task is to:\n",
        "\n",
        "  - Develop a Python script to parse and separate each question and its answer from the entry.\n",
        "  - Save each question-answer pair in a structured JSON format as follows:\n",
        "    ```json\n",
        "    {\n",
        "      \"id\": \"<unique_identifier>\",\n",
        "      \"Question\": \"<question_text>\",\n",
        "      \"Answer\": \"<answer_text>\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "  - You are encouraged to introduce additional attributes if needed to preserve the integrity and completeness of the information. Adding relevant tag information is strongly recommended.\n",
        "- **Automation Requirement:** The task must be completed using Python. Manual editing or data manipulation is strictly prohibited. Your script should efficiently handle variations in data format within the column.\n",
        "\n",
        "#### 2. Deliverables\n",
        "\n",
        "- **Reformatted Dataset:** Provide the schema of the final format you adopted for saving the results.\n",
        "- **Transformation Code:** Submit the complete code used for converting the dataset into the designated format.\n",
        "- **Statistics:** Report the total number of question-answer pairs extracted from the dataset.\n",
        "- **Performance Metrics:** Document the time taken to complete the dataset cleanup and transformation process.\n"
      ],
      "metadata": {
        "id": "hD4q_bFhvkT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "é‡‘èæ•°æ®é›†å¤„ç†å™¨ - AdaptLLM/finance-tasks æ ‡é¢˜å­é›†å¤„ç†å·¥å…·\n",
        "==========================================================\n",
        "\n",
        "è¿™ä¸ªè„šæœ¬ä¸“é—¨ç”¨äºå¤„ç†AdaptLLM/finance-tasksæ•°æ®é›†çš„Headlineå­é›†ï¼Œ\n",
        "å®ƒèƒ½å¤Ÿä»æ–‡æœ¬ä¸­æå–é—®ç­”å¯¹ï¼Œå¹¶å°†å…¶é‡æ–°æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–çš„JSONæ ¼å¼ï¼Œ\n",
        "ä¾¿äºåç»­ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒã€‚\n",
        "\n",
        "ä¸»è¦åŠŸèƒ½:\n",
        "1. ä»HuggingFaceåŠ è½½æŒ‡å®šæ•°æ®é›†\n",
        "2. ä½¿ç”¨å¤šç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æå–é—®ç­”å¯¹\n",
        "3. æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬å†…å®¹\n",
        "4. ç”Ÿæˆç»“æ„åŒ–çš„JSONè¾“å‡º\n",
        "5. æä¾›è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§\n",
        "\n",
        "ä½œè€…: æ•°æ®é›†å¤„ç†åŠ©æ‰‹\n",
        "æ—¥æœŸ: 2025-05-24\n",
        "ç‰ˆæœ¬: 1.0\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "\n",
        "# ============================================================================\n",
        "# 1. æ—¥å¿—é…ç½®å’Œå‚æ•°è®¾ç½®æ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetConfig:\n",
        "    \"\"\"\n",
        "    æ•°æ®é›†é…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰å¤„ç†å‚æ•°\n",
        "\n",
        "    è¿™ä¸ªç±»åŒ…å«äº†æ•´ä¸ªå¤„ç†æµç¨‹ä¸­éœ€è¦çš„æ‰€æœ‰é…ç½®å‚æ•°ï¼Œ\n",
        "    åŒ…æ‹¬æ•°æ®é›†åç§°ã€æ–‡ä»¶è·¯å¾„ã€æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ç­‰ã€‚\n",
        "    ç»Ÿä¸€ç®¡ç†ä¾¿äºç»´æŠ¤å’Œä¿®æ”¹ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    # æ•°æ®é›†åŸºæœ¬ä¿¡æ¯\n",
        "    DATASET_NAME = \"AdaptLLM/finance-tasks\"  # HuggingFaceä¸Šçš„æ•°æ®é›†åç§°\n",
        "    SUBSET_NAME = \"headline\"                 # è¦å¤„ç†çš„å­é›†åç§°\n",
        "    OUTPUT_FILE = \"finance_headline_qa_pairs.json\"  # è¾“å‡ºæ–‡ä»¶å\n",
        "    LOG_FILE = \"processing.log\"             # æ—¥å¿—æ–‡ä»¶å\n",
        "\n",
        "    # é—®ç­”æå–çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼\n",
        "    # è¿™äº›æ¨¡å¼ç”¨äºè¯†åˆ«æ–‡æœ¬ä¸­çš„é—®ç­”å¯¹ï¼Œæ”¯æŒå¤šç§å¸¸è§æ ¼å¼\n",
        "    QA_PATTERNS = [\n",
        "        r'Q:\\s*(.*?)\\s*A:\\s*(Yes|No)',      # Q: é—®é¢˜ A: ç­”æ¡ˆ æ ¼å¼\n",
        "        r'Question:\\s*(.*?)\\s*Answer:\\s*(Yes|No)',  # Question: é—®é¢˜ Answer: ç­”æ¡ˆ æ ¼å¼\n",
        "        r'(\\d+\\.\\s*.*?)\\s*(Yes|No)',        # æ•°å­—ç¼–å· + é—®é¢˜ + ç­”æ¡ˆ æ ¼å¼\n",
        "        r'(.*?\\?)\\s*(Yes|No)',              # ä»¥é—®å·ç»“å°¾çš„é—®é¢˜ + ç­”æ¡ˆ æ ¼å¼\n",
        "    ]\n",
        "\n",
        "def setup_logging() -> logging.Logger:\n",
        "    \"\"\"\n",
        "    è®¾ç½®æ—¥å¿—ç³»ç»Ÿé…ç½®\n",
        "\n",
        "    é…ç½®æ—¥å¿—è®°å½•å™¨ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶å°†æ—¥å¿—ä¿¡æ¯è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°ã€‚\n",
        "    è¿™æ ·æ—¢èƒ½å®æ—¶æŸ¥çœ‹å¤„ç†è¿›åº¦ï¼Œåˆèƒ½ä¿å­˜å®Œæ•´çš„å¤„ç†è®°å½•ã€‚\n",
        "\n",
        "    è¿”å›:\n",
        "        logging.Logger: é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨å®ä¾‹\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,  # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºINFO\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',  # æ—¥å¿—æ ¼å¼\n",
        "        handlers=[\n",
        "            logging.FileHandler(DatasetConfig.LOG_FILE),  # æ–‡ä»¶è¾“å‡º\n",
        "            logging.StreamHandler()  # æ§åˆ¶å°è¾“å‡º\n",
        "        ]\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. æ•°æ®åŠ è½½æ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetLoader:\n",
        "    \"\"\"\n",
        "    æ•°æ®é›†åŠ è½½å™¨ - è´Ÿè´£ä»HuggingFaceåŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†\n",
        "\n",
        "    è¿™ä¸ªç±»å°è£…äº†æ•°æ®é›†åŠ è½½çš„æ‰€æœ‰æ“ä½œï¼ŒåŒ…æ‹¬ï¼š\n",
        "    - ä»HuggingFace datasetsåº“åŠ è½½æŒ‡å®šæ•°æ®é›†\n",
        "    - éªŒè¯æ•°æ®é›†ç»“æ„çš„å®Œæ•´æ€§\n",
        "    - è½¬æ¢ä¸ºpandas DataFrameä¾¿äºåç»­å¤„ç†\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨\n",
        "\n",
        "        å‚æ•°:\n",
        "            logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "\n",
        "    def load_finance_dataset(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        åŠ è½½AdaptLLM/finance-tasksæ•°æ®é›†çš„headlineå­é›†\n",
        "\n",
        "        è¿™ä¸ªæ–¹æ³•ä½¿ç”¨HuggingFaceçš„datasetsåº“æ¥åŠ è½½æŒ‡å®šçš„æ•°æ®é›†ï¼Œ\n",
        "        ç„¶åå°†å…¶è½¬æ¢ä¸ºpandas DataFrameæ ¼å¼ï¼Œä¾¿äºåç»­çš„æ•°æ®å¤„ç†æ“ä½œã€‚\n",
        "\n",
        "        è¿”å›:\n",
        "            pd.DataFrame: åŠ è½½åçš„æ•°æ®é›†DataFrame\n",
        "\n",
        "        å¼‚å¸¸:\n",
        "            Exception: å½“æ•°æ®é›†åŠ è½½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"å¼€å§‹åŠ è½½æ•°æ®é›†: {DatasetConfig.DATASET_NAME}\")\n",
        "            self.logger.info(f\"ç›®æ ‡å­é›†: {DatasetConfig.SUBSET_NAME}\")\n",
        "\n",
        "            # ä½¿ç”¨HuggingFace datasetsåº“åŠ è½½æ•°æ®é›†\n",
        "            # split='train' è¡¨ç¤ºåŠ è½½è®­ç»ƒé›†éƒ¨åˆ†\n",
        "            dataset = load_dataset(\n",
        "                DatasetConfig.DATASET_NAME,\n",
        "                DatasetConfig.SUBSET_NAME,\n",
        "                split='train'\n",
        "            )\n",
        "\n",
        "            # å°†HuggingFace Datasetå¯¹è±¡è½¬æ¢ä¸ºpandas DataFrame\n",
        "            # è¿™æ ·å¯ä»¥åˆ©ç”¨pandaså¼ºå¤§çš„æ•°æ®å¤„ç†åŠŸèƒ½\n",
        "            df = dataset.to_pandas()\n",
        "\n",
        "            self.logger.info(f\"æ•°æ®é›†åŠ è½½æˆåŠŸ! æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
        "            self.logger.info(f\"æ•°æ®åˆ—å: {list(df.columns)}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def validate_dataset_structure(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        éªŒè¯æ•°æ®é›†ç»“æ„æ˜¯å¦ç¬¦åˆé¢„æœŸ\n",
        "\n",
        "        æ£€æŸ¥æ•°æ®é›†æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—ï¼Œä»¥åŠæ•°æ®çš„åŸºæœ¬å®Œæ•´æ€§ã€‚\n",
        "        è¿™æ˜¯æ•°æ®å¤„ç†å‰çš„é‡è¦éªŒè¯æ­¥éª¤ï¼Œç¡®ä¿åç»­å¤„ç†ä¸ä¼šå‡ºé”™ã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            df (pd.DataFrame): è¦éªŒè¯çš„æ•°æ®é›†\n",
        "\n",
        "        è¿”å›:\n",
        "            bool: éªŒè¯é€šè¿‡è¿”å›True\n",
        "\n",
        "        å¼‚å¸¸:\n",
        "            ValueError: å½“æ•°æ®é›†ç»“æ„ä¸ç¬¦åˆè¦æ±‚æ—¶æŠ›å‡ºå¼‚å¸¸\n",
        "        \"\"\"\n",
        "        # å®šä¹‰å¿…éœ€çš„åˆ—å\n",
        "        required_columns = ['input']\n",
        "\n",
        "        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„åˆ—\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "            raise ValueError(f\"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}\")\n",
        "\n",
        "        # æ£€æŸ¥inputåˆ—æ˜¯å¦å®Œå…¨ä¸ºç©º\n",
        "        if df['input'].isnull().all():\n",
        "            raise ValueError(\"inputåˆ—å®Œå…¨ä¸ºç©ºï¼Œæ— æ³•å¤„ç†\")\n",
        "\n",
        "        self.logger.info(\"æ•°æ®é›†ç»“æ„éªŒè¯é€šè¿‡\")\n",
        "        return True\n",
        "\n",
        "# ============================================================================\n",
        "# 3. é—®ç­”æå–æ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "class QuestionAnswerExtractor:\n",
        "    \"\"\"\n",
        "    é—®ç­”å¯¹æå–å™¨ - æ ¸å¿ƒå¤„ç†æ¨¡å—\n",
        "\n",
        "    è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä»åŸå§‹æ–‡æœ¬ä¸­æå–é—®ç­”å¯¹ã€‚\n",
        "    ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\n",
        "    - ä½¿ç”¨å¤šç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…é—®ç­”å¯¹\n",
        "    - æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬å†…å®¹\n",
        "    - éªŒè¯æå–ç»“æœçš„è´¨é‡\n",
        "    - å»é™¤é‡å¤çš„é—®ç­”å¯¹\n",
        "    - ç”Ÿæˆç»“æ„åŒ–çš„è¾“å‡ºè®°å½•\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–é—®ç­”æå–å™¨\n",
        "\n",
        "        å‚æ•°:\n",
        "            logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        # ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼Œç”¨äºè·Ÿè¸ªå¤„ç†æ•ˆæœ\n",
        "        self.extraction_stats = {\n",
        "            'total_entries': 0,        # æ€»å¤„ç†æ¡ç›®æ•°\n",
        "            'successful_extractions': 0,  # æˆåŠŸæå–çš„æ¡ç›®æ•°\n",
        "            'failed_extractions': 0,   # æå–å¤±è´¥çš„æ¡ç›®æ•°\n",
        "            'total_qa_pairs': 0        # æ€»é—®ç­”å¯¹æ•°é‡\n",
        "        }\n",
        "\n",
        "    def extract_qa_pairs(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        ä»ç»™å®šæ–‡æœ¬ä¸­æå–é—®ç­”å¯¹\n",
        "\n",
        "        è¿™æ˜¯æ ¸å¿ƒæå–æ–¹æ³•ï¼Œä½¿ç”¨å¤šç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥è¯†åˆ«æ–‡æœ¬ä¸­çš„é—®ç­”å¯¹ã€‚\n",
        "        æ–¹æ³•ä¼šå°è¯•æ‰€æœ‰é¢„å®šä¹‰çš„æ¨¡å¼ï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦åœ°æå–æœ‰æ•ˆä¿¡æ¯ã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            text (str): åŒ…å«é—®ç­”å¯¹çš„è¾“å…¥æ–‡æœ¬\n",
        "\n",
        "        è¿”å›:\n",
        "            List[Dict[str, str]]: æå–åˆ°çš„é—®ç­”å¯¹åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        # å¤„ç†ç©ºå€¼æƒ…å†µ\n",
        "        if not text or pd.isna(text):\n",
        "            return []\n",
        "\n",
        "        qa_pairs = []\n",
        "        text_cleaned = self._clean_text(text)  # æ¸…ç†æ–‡æœ¬\n",
        "\n",
        "        # å°è¯•ä½¿ç”¨æ¯ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¿›è¡ŒåŒ¹é…\n",
        "        for pattern in DatasetConfig.QA_PATTERNS:\n",
        "            matches = re.findall(pattern, text_cleaned, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "            # å¤„ç†æ¯ä¸ªåŒ¹é…ç»“æœ\n",
        "            for match in matches:\n",
        "                if len(match) == 2:  # ç¡®ä¿åŒ¹é…åˆ°é—®é¢˜å’Œç­”æ¡ˆä¸¤éƒ¨åˆ†\n",
        "                    question, answer = match\n",
        "                    question = self._clean_question(question.strip())\n",
        "                    answer = answer.strip()\n",
        "\n",
        "                    # éªŒè¯é—®ç­”å¯¹çš„æœ‰æ•ˆæ€§\n",
        "                    if self._validate_qa_pair(question, answer):\n",
        "                        qa_pairs.append({\n",
        "                            'question': question,\n",
        "                            'answer': answer\n",
        "                        })\n",
        "\n",
        "        # å»é™¤é‡å¤çš„é—®ç­”å¯¹ï¼Œä½†ä¿æŒåŸæœ‰é¡ºåº\n",
        "        qa_pairs = self._remove_duplicate_pairs(qa_pairs)\n",
        "\n",
        "        return qa_pairs\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        æ¸…ç†å’Œæ ‡å‡†åŒ–è¾“å…¥æ–‡æœ¬\n",
        "\n",
        "        å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬ï¼š\n",
        "        - è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦\n",
        "        - ç»Ÿä¸€é—®ç­”æ ‡è®°æ ¼å¼\n",
        "        - å»é™¤å¤šä½™çš„ç¬¦å·\n",
        "\n",
        "        å‚æ•°:\n",
        "            text (str): åŸå§‹è¾“å…¥æ–‡æœ¬\n",
        "\n",
        "        è¿”å›:\n",
        "            str: æ¸…ç†åçš„æ–‡æœ¬\n",
        "        \"\"\"\n",
        "        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦ï¼Œå°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # æ ‡å‡†åŒ–é—®é¢˜æ ‡è®°æ ¼å¼\n",
        "        text = re.sub(r'Q\\d*[\\.:]\\s*', 'Q: ', text)        # Q1: -> Q:\n",
        "        text = re.sub(r'Question\\d*[\\.:]\\s*', 'Question: ', text)  # Question1: -> Question:\n",
        "        text = re.sub(r'A\\d*[\\.:]\\s*', 'A: ', text)        # A1: -> A:\n",
        "        text = re.sub(r'Answer\\d*[\\.:]\\s*', 'Answer: ', text)      # Answer1: -> Answer:\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _clean_question(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        æ¸…ç†å’Œæ ¼å¼åŒ–é—®é¢˜æ–‡æœ¬\n",
        "\n",
        "        å¯¹æå–å‡ºçš„é—®é¢˜æ–‡æœ¬è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ï¼š\n",
        "        - å»é™¤ç¼–å·å’Œé¡¹ç›®ç¬¦å·\n",
        "        - ä¸ºç–‘é—®å¥æ·»åŠ é—®å·ï¼ˆå¦‚æœç¼ºå°‘ï¼‰\n",
        "        - ç»Ÿä¸€æ ¼å¼\n",
        "\n",
        "        å‚æ•°:\n",
        "            question (str): åŸå§‹é—®é¢˜æ–‡æœ¬\n",
        "\n",
        "        è¿”å›:\n",
        "            str: æ¸…ç†åçš„é—®é¢˜æ–‡æœ¬\n",
        "        \"\"\"\n",
        "        # å»é™¤å¼€å¤´çš„æ•°å­—ç¼–å·å’Œé¡¹ç›®ç¬¦å·\n",
        "        question = re.sub(r'^\\d+[\\.\\)]\\s*', '', question)  # 1. æˆ– 1)\n",
        "        question = re.sub(r'^[â€¢\\-\\*]\\s*', '', question)    # â€¢ - *\n",
        "\n",
        "        # å¦‚æœæ˜¯ç–‘é—®å¥ä½†æ²¡æœ‰é—®å·ï¼Œåˆ™æ·»åŠ é—®å·\n",
        "        # é€šè¿‡æ£€æŸ¥æ˜¯å¦åŒ…å«ç–‘é—®è¯æ¥åˆ¤æ–­\n",
        "        question_words = ['is', 'are', 'was', 'were', 'do', 'does', 'did',\n",
        "                         'can', 'could', 'will', 'would', 'should', 'has', 'have']\n",
        "\n",
        "        if (not question.endswith('?') and\n",
        "            any(word in question.lower() for word in question_words)):\n",
        "            question += '?'\n",
        "\n",
        "        return question.strip()\n",
        "\n",
        "    def _validate_qa_pair(self, question: str, answer: str) -> bool:\n",
        "        \"\"\"\n",
        "        éªŒè¯æå–çš„é—®ç­”å¯¹æ˜¯å¦æœ‰æ•ˆ\n",
        "\n",
        "        å¯¹æå–çš„é—®ç­”å¯¹è¿›è¡Œè´¨é‡æ£€æŸ¥ï¼ŒåŒ…æ‹¬ï¼š\n",
        "        - æ£€æŸ¥é•¿åº¦æ˜¯å¦åˆç†\n",
        "        - éªŒè¯ç­”æ¡ˆæ˜¯å¦ä¸ºYes/No\n",
        "        - é¿å…æ— æ„ä¹‰çš„é—®é¢˜\n",
        "\n",
        "        å‚æ•°:\n",
        "            question (str): é—®é¢˜æ–‡æœ¬\n",
        "            answer (str): ç­”æ¡ˆæ–‡æœ¬\n",
        "\n",
        "        è¿”å›:\n",
        "            bool: æœ‰æ•ˆè¿”å›Trueï¼Œå¦åˆ™è¿”å›False\n",
        "        \"\"\"\n",
        "        # æ£€æŸ¥æœ€å°é•¿åº¦è¦æ±‚\n",
        "        if len(question.strip()) < 5 or len(answer.strip()) < 2:\n",
        "            return False\n",
        "\n",
        "        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸ºYesæˆ–No\n",
        "        if answer.lower() not in ['yes', 'no']:\n",
        "            return False\n",
        "\n",
        "        # é¿å…æ— æ„ä¹‰æˆ–é‡å¤çš„é—®é¢˜\n",
        "        if question.lower() in ['yes', 'no', 'question', 'answer']:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _remove_duplicate_pairs(self, qa_pairs: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        å»é™¤é‡å¤çš„é—®ç­”å¯¹\n",
        "\n",
        "        ä½¿ç”¨å“ˆå¸Œå€¼æ¥è¯†åˆ«é‡å¤çš„é—®ç­”å¯¹ï¼Œç¡®ä¿ç»“æœä¸­ä¸åŒ…å«é‡å¤å†…å®¹ï¼Œ\n",
        "        åŒæ—¶ä¿æŒåŸæœ‰çš„é¡ºåºã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            qa_pairs (List[Dict[str, str]]): é—®ç­”å¯¹åˆ—è¡¨\n",
        "\n",
        "        è¿”å›:\n",
        "            List[Dict[str, str]]: å»é‡åçš„é—®ç­”å¯¹åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        seen = set()  # ç”¨äºå­˜å‚¨å·²è§è¿‡çš„å“ˆå¸Œå€¼\n",
        "        unique_pairs = []\n",
        "\n",
        "        for pair in qa_pairs:\n",
        "            # ä¸ºé—®ç­”å¯¹åˆ›å»ºå”¯ä¸€çš„å“ˆå¸Œæ ‡è¯†\n",
        "            pair_content = f\"{pair['question'].lower()}-{pair['answer'].lower()}\"\n",
        "            pair_hash = hashlib.md5(pair_content.encode()).hexdigest()\n",
        "\n",
        "            # å¦‚æœæ˜¯æ–°çš„é—®ç­”å¯¹ï¼Œåˆ™æ·»åŠ åˆ°ç»“æœä¸­\n",
        "            if pair_hash not in seen:\n",
        "                seen.add(pair_hash)\n",
        "                unique_pairs.append(pair)\n",
        "\n",
        "        return unique_pairs\n",
        "\n",
        "    def process_dataset(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œæå–æ‰€æœ‰é—®ç­”å¯¹\n",
        "\n",
        "        è¿™æ˜¯æ•°æ®é›†çº§åˆ«çš„å¤„ç†æ–¹æ³•ï¼Œéå†æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œï¼Œ\n",
        "        æå–å…¶ä¸­çš„é—®ç­”å¯¹ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„è®°å½•ã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            df (pd.DataFrame): è¾“å…¥æ•°æ®é›†\n",
        "\n",
        "        è¿”å›:\n",
        "            List[Dict[str, Any]]: ç»“æ„åŒ–çš„é—®ç­”è®°å½•åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        all_qa_records = []\n",
        "        self.extraction_stats['total_entries'] = len(df)\n",
        "\n",
        "        self.logger.info(f\"å¼€å§‹å¤„ç† {len(df)} æ¡æ•°æ®é›†æ¡ç›®...\")\n",
        "\n",
        "        # éå†æ•°æ®é›†çš„æ¯ä¸€è¡Œ\n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                input_text = row['input']\n",
        "                qa_pairs = self.extract_qa_pairs(input_text)\n",
        "\n",
        "                if qa_pairs:\n",
        "                    self.extraction_stats['successful_extractions'] += 1\n",
        "\n",
        "                    # ä¸ºæ¯ä¸ªé—®ç­”å¯¹åˆ›å»ºç»“æ„åŒ–è®°å½•\n",
        "                    for qa_idx, qa_pair in enumerate(qa_pairs):\n",
        "                        record = self._create_qa_record(\n",
        "                            original_idx=idx,\n",
        "                            qa_idx=qa_idx,\n",
        "                            question=qa_pair['question'],\n",
        "                            answer=qa_pair['answer'],\n",
        "                            source_row=row\n",
        "                        )\n",
        "                        all_qa_records.append(record)\n",
        "                        self.extraction_stats['total_qa_pairs'] += 1\n",
        "                else:\n",
        "                    self.extraction_stats['failed_extractions'] += 1\n",
        "                    self.logger.debug(f\"åœ¨æ¡ç›® {idx} ä¸­æœªæ‰¾åˆ°é—®ç­”å¯¹\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.extraction_stats['failed_extractions'] += 1\n",
        "                self.logger.error(f\"å¤„ç†æ¡ç›® {idx} æ—¶å‡ºé”™: {str(e)}\")\n",
        "\n",
        "        self.logger.info(f\"æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_qa_records)} ä¸ªé—®ç­”å¯¹\")\n",
        "        return all_qa_records\n",
        "\n",
        "    def _create_qa_record(self, original_idx: int, qa_idx: int, question: str,\n",
        "                         answer: str, source_row: pd.Series) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        åˆ›å»ºç»“æ„åŒ–çš„é—®ç­”è®°å½•\n",
        "\n",
        "        å°†æå–çš„é—®ç­”å¯¹è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„JSONè®°å½•æ ¼å¼ï¼Œ\n",
        "        åŒ…å«é—®é¢˜ã€ç­”æ¡ˆä»¥åŠç›¸å…³çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            original_idx (int): åŸå§‹æ•°æ®é›†ä¸­çš„ç´¢å¼•\n",
        "            qa_idx (int): è¯¥æ¡ç›®ä¸­é—®ç­”å¯¹çš„ç´¢å¼•\n",
        "            question (str): é—®é¢˜æ–‡æœ¬\n",
        "            answer (str): ç­”æ¡ˆæ–‡æœ¬\n",
        "            source_row (pd.Series): åŸå§‹æ•°æ®è¡Œ\n",
        "\n",
        "        è¿”å›:\n",
        "            Dict[str, Any]: ç»“æ„åŒ–çš„é—®ç­”è®°å½•\n",
        "        \"\"\"\n",
        "        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦\n",
        "        unique_id = f\"finance_headline_{original_idx:06d}_{qa_idx:03d}\"\n",
        "\n",
        "        # åˆ›å»ºåŸºç¡€è®°å½•ç»“æ„\n",
        "        record = {\n",
        "            \"id\": unique_id,\n",
        "            \"Question\": question,\n",
        "            \"Answer\": answer,\n",
        "            \"metadata\": {\n",
        "                \"source_dataset\": DatasetConfig.DATASET_NAME,\n",
        "                \"subset\": DatasetConfig.SUBSET_NAME,\n",
        "                \"original_index\": original_idx,\n",
        "                \"qa_pair_index\": qa_idx,\n",
        "                \"extraction_timestamp\": datetime.now().isoformat(),\n",
        "                \"tags\": self._generate_tags(question, answer)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # æ·»åŠ æºæ•°æ®çš„å…¶ä»–å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
        "        for col in source_row.index:\n",
        "            if col not in ['input'] and pd.notna(source_row[col]):\n",
        "                record[\"metadata\"][f\"source_{col}\"] = source_row[col]\n",
        "\n",
        "        return record\n",
        "\n",
        "    def _generate_tags(self, question: str, answer: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        ä¸ºé—®ç­”å¯¹ç”Ÿæˆç›¸å…³æ ‡ç­¾\n",
        "\n",
        "        åŸºäºé—®é¢˜å†…å®¹å’Œç­”æ¡ˆï¼Œè‡ªåŠ¨ç”Ÿæˆç›¸å…³çš„æ ‡ç­¾ï¼Œ\n",
        "        è¿™äº›æ ‡ç­¾æœ‰åŠ©äºåç»­çš„åˆ†ç±»å’Œæ£€ç´¢ã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            question (str): é—®é¢˜æ–‡æœ¬\n",
        "            answer (str): ç­”æ¡ˆæ–‡æœ¬\n",
        "\n",
        "        è¿”å›:\n",
        "            List[str]: ç›¸å…³æ ‡ç­¾åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        # åŸºç¡€æ ‡ç­¾\n",
        "        tags = [\"finance\", \"headline\", \"binary_classification\"]\n",
        "\n",
        "        # åŸºäºç­”æ¡ˆçš„æ ‡ç­¾\n",
        "        tags.append(f\"answer_{answer.lower()}\")\n",
        "\n",
        "        # åŸºäºå†…å®¹çš„æ ‡ç­¾åˆ†æ\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # ä»·æ ¼ç›¸å…³\n",
        "        if any(word in question_lower for word in ['price', 'cost', 'dollar', '$', 'money']):\n",
        "            tags.append(\"pricing\")\n",
        "\n",
        "        # å…¬å¸ç›¸å…³\n",
        "        if any(word in question_lower for word in ['company', 'corporation', 'firm', 'business']):\n",
        "            tags.append(\"corporate\")\n",
        "\n",
        "        # å¸‚åœºç›¸å…³\n",
        "        if any(word in question_lower for word in ['market', 'stock', 'share', 'trading']):\n",
        "            tags.append(\"market\")\n",
        "\n",
        "        # è´¢åŠ¡è¡¨ç°ç›¸å…³\n",
        "        if any(word in question_lower for word in ['revenue', 'profit', 'earnings', 'income']):\n",
        "            tags.append(\"financial_performance\")\n",
        "\n",
        "        # è¶‹åŠ¿åˆ†æç›¸å…³\n",
        "        if any(word in question_lower for word in ['increase', 'decrease', 'rise', 'fall', 'growth']):\n",
        "            tags.append(\"trend_analysis\")\n",
        "\n",
        "        return tags\n",
        "\n",
        "    def get_extraction_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        è·å–æå–è¿‡ç¨‹çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n",
        "\n",
        "        è®¡ç®—å¹¶è¿”å›æå–è¿‡ç¨‹çš„å„ç§ç»Ÿè®¡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æˆåŠŸç‡ã€\n",
        "        å¹³å‡é—®ç­”å¯¹æ•°é‡ç­‰ï¼Œç”¨äºè¯„ä¼°å¤„ç†æ•ˆæœã€‚\n",
        "\n",
        "        è¿”å›:\n",
        "            Dict[str, Any]: æå–ç»Ÿè®¡ä¿¡æ¯\n",
        "        \"\"\"\n",
        "        # è®¡ç®—æˆåŠŸç‡\n",
        "        success_rate = (self.extraction_stats['successful_extractions'] /\n",
        "                       max(1, self.extraction_stats['total_entries'])) * 100\n",
        "\n",
        "        # è®¡ç®—å¹³å‡é—®ç­”å¯¹æ•°é‡\n",
        "        avg_pairs_per_entry = (self.extraction_stats['total_qa_pairs'] /\n",
        "                              max(1, self.extraction_stats['successful_extractions']))\n",
        "\n",
        "        return {\n",
        "            **self.extraction_stats,\n",
        "            'success_rate_percent': round(success_rate, 2),\n",
        "            'average_pairs_per_successful_entry': round(avg_pairs_per_entry, 2)\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 4. è¾“å‡ºå’Œåºåˆ—åŒ–æ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetSerializer:\n",
        "    \"\"\"\n",
        "    æ•°æ®é›†åºåˆ—åŒ–å™¨ - è´Ÿè´£ä¿å­˜å’Œæ ¼å¼åŒ–å¤„ç†ç»“æœ\n",
        "\n",
        "    è¿™ä¸ªç±»å¤„ç†æ‰€æœ‰ä¸æ•°æ®è¾“å‡ºç›¸å…³çš„æ“ä½œï¼š\n",
        "    - å°†é—®ç­”è®°å½•ä¿å­˜ä¸ºJSONæ–‡ä»¶\n",
        "    - ç”Ÿæˆæ•°æ®æ ¼å¼çš„æ–‡æ¡£è¯´æ˜\n",
        "    - ç¡®ä¿è¾“å‡ºæ ¼å¼çš„ä¸€è‡´æ€§å’Œå¯è¯»æ€§\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–åºåˆ—åŒ–å™¨\n",
        "\n",
        "        å‚æ•°:\n",
        "            logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "\n",
        "    def save_qa_records(self, qa_records: List[Dict[str, Any]],\n",
        "                       output_file: str = None) -> str:\n",
        "        \"\"\"\n",
        "        ä¿å­˜é—®ç­”è®°å½•åˆ°JSONæ–‡ä»¶\n",
        "\n",
        "        å°†å¤„ç†å¥½çš„é—®ç­”è®°å½•ä»¥JSONæ ¼å¼ä¿å­˜åˆ°æ–‡ä»¶ï¼Œ\n",
        "        ä½¿ç”¨UTF-8ç¼–ç ç¡®ä¿ä¸­æ–‡ç­‰ç‰¹æ®Šå­—ç¬¦æ­£ç¡®æ˜¾ç¤ºã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            qa_records (List[Dict[str, Any]]): é—®ç­”è®°å½•åˆ—è¡¨\n",
        "            output_file (str, optional): è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
        "\n",
        "        è¿”å›:\n",
        "            str: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„\n",
        "        \"\"\"\n",
        "        if output_file is None:\n",
        "            output_file = DatasetConfig.OUTPUT_FILE\n",
        "\n",
        "        try:\n",
        "            # ä»¥UTF-8ç¼–ç ä¿å­˜JSONæ–‡ä»¶ï¼Œç¡®ä¿æ ¼å¼ç¾è§‚\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_records, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            self.logger.info(f\"æˆåŠŸä¿å­˜ {len(qa_records)} æ¡è®°å½•åˆ° {output_file}\")\n",
        "            return output_file\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"ä¿å­˜è®°å½•å¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_schema_documentation(self, qa_records: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        ç”Ÿæˆè¾“å‡ºæ ¼å¼çš„æ–‡æ¡£è¯´æ˜\n",
        "\n",
        "        åˆ›å»ºè¯¦ç»†çš„æ•°æ®æ ¼å¼æ–‡æ¡£ï¼ŒåŒ…æ‹¬å­—æ®µè¯´æ˜ã€æ•°æ®ç±»å‹ã€\n",
        "        ç¤ºä¾‹è®°å½•ç­‰ï¼Œä¾¿äºç†è§£å’Œä½¿ç”¨è¾“å‡ºæ•°æ®ã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            qa_records (List[Dict[str, Any]]): æ ·æœ¬è®°å½•\n",
        "\n",
        "        è¿”å›:\n",
        "            Dict[str, Any]: æ ¼å¼æ–‡æ¡£\n",
        "        \"\"\"\n",
        "        schema = {\n",
        "            \"format_version\": \"1.0\",\n",
        "            \"description\": \"ä»AdaptLLM/finance-tasks headlineå­é›†æå–çš„ç»“æ„åŒ–é—®ç­”å¯¹\",\n",
        "            \"record_structure\": {\n",
        "                \"id\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"é—®ç­”å¯¹çš„å”¯ä¸€æ ‡è¯†ç¬¦\",\n",
        "                    \"pattern\": \"finance_headline_XXXXXX_XXX\",\n",
        "                    \"example\": \"finance_headline_000001_001\"\n",
        "                },\n",
        "                \"Question\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"æå–çš„é—®é¢˜æ–‡æœ¬\"\n",
        "                },\n",
        "                \"Answer\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"é—®é¢˜çš„ç­”æ¡ˆï¼ˆYesæˆ–Noï¼‰\",\n",
        "                    \"enum\": [\"Yes\", \"No\"]\n",
        "                },\n",
        "                \"metadata\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"description\": \"è®°å½•çš„é™„åŠ å…ƒæ•°æ®ä¿¡æ¯\",\n",
        "                    \"properties\": {\n",
        "                        \"source_dataset\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"æºæ•°æ®é›†åç§°\"\n",
        "                        },\n",
        "                        \"subset\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"æ•°æ®é›†å­é›†åç§°\"\n",
        "                        },\n",
        "                        \"original_index\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"åœ¨åŸå§‹æ•°æ®é›†ä¸­çš„ç´¢å¼•\"\n",
        "                        },\n",
        "                        \"qa_pair_index\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"åœ¨è¯¥æ¡ç›®ä¸­çš„é—®ç­”å¯¹ç´¢å¼•\"\n",
        "                        },\n",
        "                        \"extraction_timestamp\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"format\": \"ISO8601\",\n",
        "                            \"description\": \"æå–æ—¶é—´æˆ³\"\n",
        "                        },\n",
        "                        \"tags\": {\n",
        "                            \"type\": \"array\",\n",
        "                            \"items\": {\"type\": \"string\"},\n",
        "                            \"description\": \"è‡ªåŠ¨ç”Ÿæˆçš„å†…å®¹æ ‡ç­¾\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"sample_record\": qa_records[0] if qa_records else None,\n",
        "            \"total_records\": len(qa_records),\n",
        "            \"usage_notes\": [\n",
        "                \"æ¯ä¸ªè®°å½•åŒ…å«ä¸€ä¸ªé—®é¢˜å’Œå¯¹åº”çš„Yes/Noç­”æ¡ˆ\",\n",
        "                \"idå­—æ®µç¡®ä¿æ¯ä¸ªé—®ç­”å¯¹çš„å”¯ä¸€æ€§\",\n",
        "                \"metadataåŒ…å«ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯\",\n",
        "                \"tagså­—æ®µä¾¿äºåˆ†ç±»å’Œæ£€ç´¢\",\n",
        "                \"æ‰€æœ‰æ–‡æœ¬å‡ç»è¿‡æ¸…ç†å’Œæ ‡å‡†åŒ–å¤„ç†\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return schema\n",
        "\n",
        "# ============================================================================\n",
        "# 5. æ€§èƒ½ç›‘æ§æ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"\n",
        "    æ€§èƒ½ç›‘æ§å™¨ - è·Ÿè¸ªå’ŒæŠ¥å‘Šå¤„ç†æ€§èƒ½\n",
        "\n",
        "    ç›‘æ§æ•´ä¸ªå¤„ç†æµç¨‹çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š\n",
        "    - æ€»å¤„ç†æ—¶é—´\n",
        "    - å„ä¸ªé˜¶æ®µçš„è€—æ—¶\n",
        "    - å¤„ç†é€Ÿåº¦ç»Ÿè®¡\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨\n",
        "\n",
        "        å‚æ•°:\n",
        "            logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        self.start_time = None      # å¼€å§‹æ—¶é—´\n",
        "        self.end_time = None        # ç»“æŸæ—¶é—´\n",
        "        self.checkpoints = {}       # æ£€æŸ¥ç‚¹æ—¶é—´è®°å½•\n",
        "\n",
        "    def start_monitoring(self):\n",
        "        \"\"\"å¼€å§‹æ€§èƒ½ç›‘æ§\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.logger.info(\"å¼€å§‹æ€§èƒ½ç›‘æ§\")\n",
        "\n",
        "    def checkpoint(self, name: str):\n",
        "        \"\"\"\n",
        "        è®°å½•æ€§èƒ½æ£€æŸ¥ç‚¹\n",
        "\n",
        "        åœ¨å¤„ç†è¿‡ç¨‹çš„å…³é”®èŠ‚ç‚¹è®°å½•æ—¶é—´ï¼Œ\n",
        "        ç”¨äºåˆ†æå„ä¸ªé˜¶æ®µçš„è€—æ—¶æƒ…å†µã€‚\n",
        "\n",
        "        å‚æ•°:\n",
        "            name (str): æ£€æŸ¥ç‚¹åç§°\n",
        "        \"\"\"\n",
        "        if self.start_time is None:\n",
        "            self.start_monitoring()\n",
        "\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        self.checkpoints[name] = elapsed_time\n",
        "        self.logger.info(f\"æ£€æŸ¥ç‚¹ '{name}': {elapsed_time:.2f} ç§’\")\n",
        "\n",
        "    def finish_monitoring(self) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        ç»“æŸç›‘æ§å¹¶è¿”å›æ€§èƒ½æŒ‡æ ‡\n",
        "\n",
        "        è®¡ç®—æ€»å¤„ç†æ—¶é—´å’Œå„é˜¶æ®µç”¨æ—¶ï¼Œ\n",
        "        ç”Ÿæˆå®Œæ•´çš„æ€§èƒ½æŠ¥å‘Šã€‚\n",
        "\n",
        "        è¿”å›:\n",
        "            Dict[str, float]: æ€§èƒ½æŒ‡æ ‡å­—å…¸\n",
        "        \"\"\"\n",
        "        self.end_time = time.time()\n",
        "        total_time = self.end_time - self.start_time\n",
        "\n",
        "        # æ„å»ºæ€§èƒ½æŒ‡æ ‡å­—å…¸\n",
        "        metrics = {\n",
        "            'total_processing_time_seconds': round(total_time, 2),\n",
        "            'total_processing_time_minutes': round(total_time / 60, 2),\n",
        "            # æ·»åŠ å„æ£€æŸ¥ç‚¹çš„ç”¨æ—¶\n",
        "            **{f\"{name}_seconds\": round(time_val, 2)\n",
        "               for name, time_val in self.checkpoints.items()}\n",
        "        }\n",
        "\n",
        "        self.logger.info(f\"æ€»å¤„ç†æ—¶é—´: {metrics['total_processing_time_seconds']} ç§’\")\n",
        "        return metrics\n",
        "\n",
        "# ============================================================================\n",
        "# 6. ä¸»è¦åè°ƒæ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "class FinanceDatasetProcessor:\n",
        "    \"\"\"\n",
        "    é‡‘èæ•°æ®é›†å¤„ç†å™¨ - ä¸»è¦åè°ƒç±»\n",
        "\n",
        "    è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒåè°ƒå™¨ï¼Œè´Ÿè´£ï¼š\n",
        "    - åè°ƒå„ä¸ªæ¨¡å—çš„å·¥ä½œ\n",
        "    - æ§åˆ¶æ•´ä¸ªå¤„ç†æµç¨‹\n",
        "    - å¤„ç†å¼‚å¸¸å’Œé”™è¯¯\n",
        "    - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š\n",
        "\n",
        "    ä½¿ç”¨ç¤ºä¾‹:\n",
        "        processor = FinanceDatasetProcessor()\n",
        "        results = processor.process_dataset()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–å¤„ç†å™¨ï¼Œåˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç»„ä»¶å®ä¾‹\n",
        "        \"\"\"\n",
        "        self.logger = setup_logging()                           # æ—¥å¿—è®°å½•å™¨\n",
        "        self.loader = DatasetLoader(self.logger)                # æ•°æ®åŠ è½½å™¨\n",
        "        self.extractor = QuestionAnswerExtractor(self.logger)   # é—®ç­”æå–å™¨\n",
        "        self.serializer = DatasetSerializer(self.logger)       # æ•°æ®åºåˆ—åŒ–å™¨\n",
        "        self.monitor = PerformanceMonitor(self.logger)          # æ€§èƒ½ç›‘æ§å™¨\n",
        "\n",
        "    def process_dataset(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        æ‰§è¡Œå®Œæ•´çš„æ•°æ®é›†å¤„ç†æµç¨‹\n",
        "\n",
        "        è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„ä¸»è¦å…¥å£æ–¹æ³•ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
        "        1. åŠ è½½æ•°æ®é›†\n",
        "        2. æå–é—®ç­”å¯¹\n",
        "        3. ä¿å­˜ç»“æœ\n",
        "        4. ç”ŸæˆæŠ¥å‘Š\n",
        "\n",
        "        è¿”å›:\n",
        "            Dict[str, Any]: åŒ…å«å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸\n",
        "        \"\"\"\n",
        "        self.logger.info(\"å¼€å§‹æ‰§è¡Œé‡‘èæ•°æ®é›†å¤„ç†æµç¨‹\")\n",
        "        self.monitor.start_monitoring()\n",
        "\n",
        "        try:\n",
        "            # æ­¥éª¤1: åŠ è½½æ•°æ®é›†\n",
        "            self.logger.info(\"æ­¥éª¤1: æ­£åœ¨åŠ è½½æ•°æ®é›†...\")\n",
        "            df = self.loader.load_finance_dataset()\n",
        "            self.loader.validate_dataset_structure(df)\n",
        "            self.monitor.checkpoint(\"æ•°æ®é›†åŠ è½½å®Œæˆ\")\n",
        "\n",
        "            # æ­¥éª¤2: æå–é—®ç­”å¯¹\n",
        "            self.logger.info(\"æ­¥éª¤2: æ­£åœ¨æå–é—®ç­”å¯¹...\")\n",
        "            qa_records = self.extractor.process_dataset(df)\n",
        "            self.monitor.checkpoint(\"é—®ç­”æå–å®Œæˆ\")\n",
        "\n",
        "            # æ­¥éª¤3: ä¿å­˜ç»“æœ\n",
        "            self.logger.info(\"æ­¥éª¤3: æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ...\")\n",
        "            output_file = self.serializer.save_qa_records(qa_records)\n",
        "            self.monitor.checkpoint(\"æ•°æ®ä¿å­˜å®Œæˆ\")\n",
        "\n",
        "            # æ­¥éª¤4: ç”Ÿæˆæ–‡æ¡£å’Œç»Ÿè®¡ä¿¡æ¯\n",
        "            self.logger.info(\"æ­¥éª¤4: æ­£åœ¨ç”Ÿæˆæ–‡æ¡£å’Œç»Ÿè®¡...\")\n",
        "            schema_doc = self.serializer.generate_schema_documentation(qa_records)\n",
        "            extraction_stats = self.extractor.get_extraction_statistics()\n",
        "            performance_metrics = self.monitor.finish_monitoring()\n",
        "\n",
        "            # ç¼–è¯‘æœ€ç»ˆç»“æœ\n",
        "            results = {\n",
        "                \"processing_summary\": {\n",
        "                    \"status\": \"å·²å®Œæˆ\",\n",
        "                    \"output_file\": output_file,\n",
        "                    \"total_qa_pairs\": len(qa_records)\n",
        "                },\n",
        "                \"extraction_statistics\": extraction_stats,\n",
        "                \"performance_metrics\": performance_metrics,\n",
        "                \"schema_documentation\": schema_doc\n",
        "            }\n",
        "\n",
        "            self.logger.info(\"æ•°æ®é›†å¤„ç†æˆåŠŸå®Œæˆ!\")\n",
        "            self._print_summary(results)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"æ•°æ®é›†å¤„ç†å¤±è´¥: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _print_summary(self, results: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        æ‰“å°æ ¼å¼åŒ–çš„å¤„ç†ç»“æœæ‘˜è¦\n",
        "\n",
        "        åœ¨æ§åˆ¶å°è¾“å‡ºç¾è§‚çš„å¤„ç†ç»“æœæ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š\n",
        "        - å¤„ç†çŠ¶æ€\n",
        "        - ç»Ÿè®¡ä¿¡æ¯\n",
        "        - æ€§èƒ½æŒ‡æ ‡\n",
        "\n",
        "        å‚æ•°:\n",
        "            results (Dict[str, Any]): å¤„ç†ç»“æœå­—å…¸\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ¦ é‡‘èæ•°æ®é›†å¤„ç†ç»“æœæ‘˜è¦\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # å¤„ç†æ‘˜è¦\n",
        "        summary = results[\"processing_summary\"]\n",
        "        print(f\"ğŸ“Š å¤„ç†çŠ¶æ€: {summary['status']}\")\n",
        "        print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶: {summary['output_file']}\")\n",
        "        print(f\"ğŸ’¬ é—®ç­”å¯¹æ€»æ•°: {summary['total_qa_pairs']:,}\")\n",
        "\n",
        "        # æå–ç»Ÿè®¡\n",
        "        stats = results[\"extraction_statistics\"]\n",
        "        print(f\"\\nğŸ“ˆ æå–ç»Ÿè®¡ä¿¡æ¯:\")\n",
        "        print(f\"  â€¢ å¤„ç†æ¡ç›®æ€»æ•°: {stats['total_entries']:,}\")\n",
        "        print(f\"  â€¢ æˆåŠŸæå–æ¡ç›®: {stats['successful_extractions']:,}\")\n",
        "        print(f\"  â€¢ æå–å¤±è´¥æ¡ç›®: {stats['failed_extractions']:,}\")\n",
        "        print(f\"  â€¢ æˆåŠŸç‡: {stats['success_rate_percent']:.1f}%\")\n",
        "        print(f\"  â€¢ å¹³å‡æ¯æ¡ç›®é—®ç­”å¯¹æ•°: {stats['average_pairs_per_successful_entry']:.1f}\")\n",
        "\n",
        "        # æ€§èƒ½æŒ‡æ ‡\n",
        "        perf = results[\"performance_metrics\"]\n",
        "        print(f\"\\nâ±ï¸  æ€§èƒ½æŒ‡æ ‡:\")\n",
        "        print(f\"  â€¢ æ€»å¤„ç†æ—¶é—´: {perf['total_processing_time_seconds']:.2f} ç§’\")\n",
        "        print(f\"  â€¢ æ€»å¤„ç†æ—¶é—´: {perf['total_processing_time_minutes']:.2f} åˆ†é’Ÿ\")\n",
        "\n",
        "        # æ˜¾ç¤ºå„é˜¶æ®µç”¨æ—¶\n",
        "        if 'æ•°æ®é›†åŠ è½½å®Œæˆ_seconds' in perf:\n",
        "            print(f\"  â€¢ æ•°æ®åŠ è½½ç”¨æ—¶: {perf['æ•°æ®é›†åŠ è½½å®Œæˆ_seconds']:.2f} ç§’\")\n",
        "        if 'é—®ç­”æå–å®Œæˆ_seconds' in perf:\n",
        "            extract_time = perf['é—®ç­”æå–å®Œæˆ_seconds'] - perf.get('æ•°æ®é›†åŠ è½½å®Œæˆ_seconds', 0)\n",
        "            print(f\"  â€¢ é—®ç­”æå–ç”¨æ—¶: {extract_time:.2f} ç§’\")\n",
        "        if 'æ•°æ®ä¿å­˜å®Œæˆ_seconds' in perf:\n",
        "            save_time = perf['æ•°æ®ä¿å­˜å®Œæˆ_seconds'] - perf.get('é—®ç­”æå–å®Œæˆ_seconds', 0)\n",
        "            print(f\"  â€¢ æ•°æ®ä¿å­˜ç”¨æ—¶: {save_time:.2f} ç§’\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"âœ… å¤„ç†å®Œæˆ! å¯ä»¥æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶è·å–è¯¦ç»†ç»“æœã€‚\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. ä¸»è¿è¡Œå‡½æ•°\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    ä¸»è¿è¡Œå‡½æ•° - ç¨‹åºå…¥å£ç‚¹\n",
        "\n",
        "    è¿™æ˜¯æ•´ä¸ªç¨‹åºçš„å…¥å£å‡½æ•°ï¼Œè´Ÿè´£ï¼š\n",
        "    1. åˆå§‹åŒ–å¤„ç†å™¨\n",
        "    2. æ‰§è¡Œå¤„ç†æµç¨‹\n",
        "    3. ä¿å­˜è¯¦ç»†ç»“æœ\n",
        "    4. å¤„ç†å¼‚å¸¸æƒ…å†µ\n",
        "\n",
        "    å¯ä»¥ç›´æ¥è¿è¡Œè„šæœ¬æˆ–å¯¼å…¥åè°ƒç”¨æ­¤å‡½æ•°ã€‚\n",
        "\n",
        "    è¿”å›:\n",
        "        Dict[str, Any]: å¤„ç†ç»“æœï¼Œå¦‚æœå¤„ç†å¤±è´¥åˆ™è¿”å›None\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ å¼€å§‹å¯åŠ¨é‡‘èæ•°æ®é›†å¤„ç†å™¨...\")\n",
        "    print(\"ğŸ“‹ ä»»åŠ¡: å¤„ç†AdaptLLM/finance-tasksæ•°æ®é›†çš„headlineå­é›†\")\n",
        "    print(\"ğŸ¯ ç›®æ ‡: æå–é—®ç­”å¯¹å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–JSONæ ¼å¼\\n\")\n",
        "\n",
        "    try:\n",
        "        # åˆå§‹åŒ–å¹¶è¿è¡Œå¤„ç†å™¨\n",
        "        processor = FinanceDatasetProcessor()\n",
        "        results = processor.process_dataset()\n",
        "\n",
        "        # ä¿å­˜è¯¦ç»†çš„å¤„ç†ç»“æœä¾›åç»­å‚è€ƒ\n",
        "        results_file = \"processing_results.json\"\n",
        "        with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"ğŸ“„ è¯¦ç»†å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {results_file}\")\n",
        "        print(\"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²æˆåŠŸå®Œæˆ!\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†å¤„ç†è¿‡ç¨‹\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
        "        print(\"ğŸ’¡ è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯\")\n",
        "        raise\n",
        "\n",
        "# ============================================================================\n",
        "# 8. è¾…åŠ©å‡½æ•° - å•ç‹¬æµ‹è¯•å„æ¨¡å—\n",
        "# ============================================================================\n",
        "\n",
        "def test_data_loading():\n",
        "    \"\"\"\n",
        "    æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½\n",
        "    ç”¨äºå•ç‹¬æµ‹è¯•æ•°æ®é›†åŠ è½½æ˜¯å¦æ­£å¸¸å·¥ä½œ\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½æ¨¡å—...\")\n",
        "    logger = setup_logging()\n",
        "    loader = DatasetLoader(logger)\n",
        "\n",
        "    try:\n",
        "        df = loader.load_finance_dataset()\n",
        "        loader.validate_dataset_structure(df)\n",
        "        print(f\"âœ… æ•°æ®åŠ è½½æµ‹è¯•æˆåŠŸ! æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def test_qa_extraction(sample_text: str = None):\n",
        "    \"\"\"\n",
        "    æµ‹è¯•é—®ç­”æå–åŠŸèƒ½\n",
        "    ç”¨äºå•ç‹¬æµ‹è¯•é—®ç­”å¯¹æå–æ˜¯å¦æ­£å¸¸å·¥ä½œ\n",
        "\n",
        "    å‚æ•°:\n",
        "        sample_text (str): æµ‹è¯•ç”¨çš„æ ·æœ¬æ–‡æœ¬\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§ª æµ‹è¯•é—®ç­”æå–æ¨¡å—...\")\n",
        "    logger = setup_logging()\n",
        "    extractor = QuestionAnswerExtractor(logger)\n",
        "\n",
        "    # ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œæµ‹è¯•\n",
        "    if sample_text is None:\n",
        "        sample_text = \"\"\"\n",
        "        Q: Is the company's revenue over $1 million? A: Yes\n",
        "        Question: Did the stock price increase? Answer: No\n",
        "        1. Was there a merger announcement? Yes\n",
        "        Does the earnings report show profit? No\n",
        "        \"\"\"\n",
        "\n",
        "    try:\n",
        "        qa_pairs = extractor.extract_qa_pairs(sample_text)\n",
        "        print(f\"âœ… é—®ç­”æå–æµ‹è¯•æˆåŠŸ! æå–åˆ° {len(qa_pairs)} ä¸ªé—®ç­”å¯¹:\")\n",
        "        for i, pair in enumerate(qa_pairs, 1):\n",
        "            print(f\"  {i}. Q: {pair['question']}\")\n",
        "            print(f\"     A: {pair['answer']}\")\n",
        "        return qa_pairs\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ é—®ç­”æå–æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def show_usage_examples():\n",
        "    \"\"\"\n",
        "    æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "ğŸ”§ ä½¿ç”¨æ–¹æ³•å’Œç¤ºä¾‹:\n",
        "\n",
        "1. ç›´æ¥è¿è¡Œè„šæœ¬:\n",
        "   python finance_dataset_processor.py\n",
        "\n",
        "2. åœ¨Pythonä»£ç ä¸­ä½¿ç”¨:\n",
        "   from finance_dataset_processor import main, FinanceDatasetProcessor\n",
        "\n",
        "   # æ–¹æ³•1: ä½¿ç”¨mainå‡½æ•°\n",
        "   results = main()\n",
        "\n",
        "   # æ–¹æ³•2: ç›´æ¥ä½¿ç”¨å¤„ç†å™¨ç±»\n",
        "   processor = FinanceDatasetProcessor()\n",
        "   results = processor.process_dataset()\n",
        "\n",
        "3. æµ‹è¯•ç‰¹å®šæ¨¡å—:\n",
        "   from finance_dataset_processor import test_data_loading, test_qa_extraction\n",
        "\n",
        "   # æµ‹è¯•æ•°æ®åŠ è½½\n",
        "   df = test_data_loading()\n",
        "\n",
        "   # æµ‹è¯•é—®ç­”æå–\n",
        "   qa_pairs = test_qa_extraction()\n",
        "\n",
        "4. è¾“å‡ºæ–‡ä»¶è¯´æ˜:\n",
        "   - finance_headline_qa_pairs.json: ä¸»è¦çš„é—®ç­”å¯¹æ•°æ®\n",
        "   - processing_results.json: è¯¦ç»†çš„å¤„ç†ç»“æœå’Œç»Ÿè®¡\n",
        "   - processing.log: å®Œæ•´çš„å¤„ç†æ—¥å¿—\n",
        "\n",
        "ğŸ“Š è¾“å‡ºJSONæ ¼å¼ç¤ºä¾‹:\n",
        "{\n",
        "  \"id\": \"finance_headline_000001_001\",\n",
        "  \"Question\": \"Does the company's revenue exceed $1 billion?\",\n",
        "  \"Answer\": \"Yes\",\n",
        "  \"metadata\": {\n",
        "    \"source_dataset\": \"AdaptLLM/finance-tasks\",\n",
        "    \"subset\": \"headline\",\n",
        "    \"original_index\": 1,\n",
        "    \"qa_pair_index\": 1,\n",
        "    \"extraction_timestamp\": \"2025-05-24T10:30:45.123456\",\n",
        "    \"tags\": [\"finance\", \"headline\", \"binary_classification\", \"answer_yes\"]\n",
        "  }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. ç¨‹åºå…¥å£ç‚¹\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°\n",
        "    import sys\n",
        "\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "-YRY45GmzbZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634,
          "referenced_widgets": [
            "e5772fa069884d7089f0cf5e70c6fbd1",
            "9427b01ff02240c0ae71d690f2e87fb1",
            "eeeaa904083a46fb95f217814f9f329e",
            "6d46ce74724c4d869c65d55a7a8bbbd9",
            "1c24edc4f85342a88bfcc184817550a7",
            "5b8b6cd8531a49519a7342ba3fddd574",
            "489b0e49949d4dc697427c58d684d2f8",
            "ba952c8e6b7f4c6eb886006d36b15bab",
            "17948f0e9cbd45d6b3e7c0c6ef192756",
            "d5b6aa82caaf4c2fbfab336171ccd29d",
            "f361658c6786488c93a91fa62be002d7"
          ]
        },
        "outputId": "19622623-5147-428d-9e8f-631849d95d0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å¼€å§‹å¯åŠ¨é‡‘èæ•°æ®é›†å¤„ç†å™¨...\n",
            "ğŸ“‹ ä»»åŠ¡: å¤„ç†AdaptLLM/finance-tasksæ•°æ®é›†çš„headlineå­é›†\n",
            "ğŸ¯ ç›®æ ‡: æå–é—®ç­”å¯¹å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–JSONæ ¼å¼\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5772fa069884d7089f0cf5e70c6fbd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:æ•°æ®é›†åŠ è½½å¤±è´¥: Invalid pattern: '**' can only be an entire path component\n",
            "ERROR:__main__:æ•°æ®é›†å¤„ç†å¤±è´¥: Invalid pattern: '**' can only be an entire path component\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: Invalid pattern: '**' can only be an entire path component\n",
            "ğŸ’¡ è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid pattern: '**' can only be an entire path component",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;31m# åˆå§‹åŒ–å¹¶è¿è¡Œå¤„ç†å™¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinanceDatasetProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;31m# ä¿å­˜è¯¦ç»†çš„å¤„ç†ç»“æœä¾›åç»­å‚è€ƒ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# æ­¥éª¤1: åŠ è½½æ•°æ®é›†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"æ­¥éª¤1: æ­£åœ¨åŠ è½½æ•°æ®é›†...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_finance_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"æ•°æ®é›†åŠ è½½å®Œæˆ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36mload_finance_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# ä½¿ç”¨HuggingFace datasetsåº“åŠ è½½æ•°æ®é›†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# split='train' è¡¨ç¤ºåŠ è½½è®­ç»ƒé›†éƒ¨åˆ†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             dataset = load_dataset(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mDatasetConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mDatasetConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUBSET_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2113\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     ) from None\n\u001b[0;32m-> 1495\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m                     \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m                     \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1480\u001b[0m         except (\n\u001b[1;32m   1481\u001b[0m             \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         )\n\u001b[1;32m   1036\u001b[0m         data_files = DataFilesDict.from_patterns(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mget_data_patterns\u001b[0;34m(base_path, download_config)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolve_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_data_files_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The directory at {base_path} doesn't contain any data files\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_path_and_storage_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fs_token_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0mfs_base_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_marker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mfs_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"expand_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     def find(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mallpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mends_with_sep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/utils.py\u001b[0m in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"**\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                 \u001b[0;34m\"Invalid pattern: '**' can only be an entire path component\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5772fa069884d7089f0cf5e70c6fbd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9427b01ff02240c0ae71d690f2e87fb1",
              "IPY_MODEL_eeeaa904083a46fb95f217814f9f329e",
              "IPY_MODEL_6d46ce74724c4d869c65d55a7a8bbbd9"
            ],
            "layout": "IPY_MODEL_1c24edc4f85342a88bfcc184817550a7"
          }
        },
        "9427b01ff02240c0ae71d690f2e87fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b8b6cd8531a49519a7342ba3fddd574",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_489b0e49949d4dc697427c58d684d2f8",
            "value": "Downloadingâ€‡readme:â€‡100%"
          }
        },
        "eeeaa904083a46fb95f217814f9f329e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba952c8e6b7f4c6eb886006d36b15bab",
            "max": 8230,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17948f0e9cbd45d6b3e7c0c6ef192756",
            "value": 8230
          }
        },
        "6d46ce74724c4d869c65d55a7a8bbbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5b6aa82caaf4c2fbfab336171ccd29d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f361658c6786488c93a91fa62be002d7",
            "value": "â€‡8.23k/8.23kâ€‡[00:00&lt;00:00,â€‡558kB/s]"
          }
        },
        "1c24edc4f85342a88bfcc184817550a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b8b6cd8531a49519a7342ba3fddd574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489b0e49949d4dc697427c58d684d2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba952c8e6b7f4c6eb886006d36b15bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17948f0e9cbd45d6b3e7c0c6ef192756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5b6aa82caaf4c2fbfab336171ccd29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f361658c6786488c93a91fa62be002d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}