{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DominicDin/teest/blob/main/%E2%80%9Cdzx_Dataset_for_LLM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take Home Test: Reformat a Public Dataset for LLM Training\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal of this task is to prepare public datasets for more effective use in training and fine-tuning Large Language Models (LLMs). You are required to reformat a specific subset of a public dataset into a structured, consistent format to facilitate its usability.\n",
        "\n",
        "### Detailed Instructions\n",
        "\n",
        "#### 1. Dataset Selection and Preparation\n",
        "\n",
        "- **Dataset:** You are assigned the `Headline` subset of the [AdaptLLM/finance-tasks](https://huggingface.co/datasets/AdaptLLM/finance-tasks) dataset.\n",
        "\n",
        "- **Task Description:** Each entry in the `input` column contains multiple \"Yes\" or \"No\" questions alongside their respective answers. Your task is to:\n",
        "\n",
        "  - Develop a Python script to parse and separate each question and its answer from the entry.\n",
        "  - Save each question-answer pair in a structured JSON format as follows:\n",
        "    ```json\n",
        "    {\n",
        "      \"id\": \"<unique_identifier>\",\n",
        "      \"Question\": \"<question_text>\",\n",
        "      \"Answer\": \"<answer_text>\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "  - You are encouraged to introduce additional attributes if needed to preserve the integrity and completeness of the information. Adding relevant tag information is strongly recommended.\n",
        "- **Automation Requirement:** The task must be completed using Python. Manual editing or data manipulation is strictly prohibited. Your script should efficiently handle variations in data format within the column.\n",
        "\n",
        "#### 2. Deliverables\n",
        "\n",
        "- **Reformatted Dataset:** Provide the schema of the final format you adopted for saving the results.\n",
        "- **Transformation Code:** Submit the complete code used for converting the dataset into the designated format.\n",
        "- **Statistics:** Report the total number of question-answer pairs extracted from the dataset.\n",
        "- **Performance Metrics:** Document the time taken to complete the dataset cleanup and transformation process.\n"
      ],
      "metadata": {
        "id": "hD4q_bFhvkT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "金融数据集处理器 - AdaptLLM/finance-tasks 标题子集处理工具\n",
        "==========================================================\n",
        "\n",
        "这个脚本专门用于处理AdaptLLM/finance-tasks数据集的Headline子集，\n",
        "它能够从文本中提取问答对，并将其重新格式化为结构化的JSON格式，\n",
        "便于后续用于大语言模型的训练和微调。\n",
        "\n",
        "主要功能:\n",
        "1. 从HuggingFace加载指定数据集\n",
        "2. 使用多种正则表达式模式提取问答对\n",
        "3. 清理和标准化文本内容\n",
        "4. 生成结构化的JSON输出\n",
        "5. 提供详细的处理统计和性能监控\n",
        "\n",
        "作者: 数据集处理助手\n",
        "日期: 2025-05-24\n",
        "版本: 1.0\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "\n",
        "# ============================================================================\n",
        "# 1. 日志配置和参数设置模块\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetConfig:\n",
        "    \"\"\"\n",
        "    数据集配置类 - 集中管理所有处理参数\n",
        "\n",
        "    这个类包含了整个处理流程中需要的所有配置参数，\n",
        "    包括数据集名称、文件路径、正则表达式模式等。\n",
        "    统一管理便于维护和修改。\n",
        "    \"\"\"\n",
        "\n",
        "    # 数据集基本信息\n",
        "    DATASET_NAME = \"AdaptLLM/finance-tasks\"  # HuggingFace上的数据集名称\n",
        "    SUBSET_NAME = \"headline\"                 # 要处理的子集名称\n",
        "    OUTPUT_FILE = \"finance_headline_qa_pairs.json\"  # 输出文件名\n",
        "    LOG_FILE = \"processing.log\"             # 日志文件名\n",
        "\n",
        "    # 问答提取的正则表达式模式\n",
        "    # 这些模式用于识别文本中的问答对，支持多种常见格式\n",
        "    QA_PATTERNS = [\n",
        "        r'Q:\\s*(.*?)\\s*A:\\s*(Yes|No)',      # Q: 问题 A: 答案 格式\n",
        "        r'Question:\\s*(.*?)\\s*Answer:\\s*(Yes|No)',  # Question: 问题 Answer: 答案 格式\n",
        "        r'(\\d+\\.\\s*.*?)\\s*(Yes|No)',        # 数字编号 + 问题 + 答案 格式\n",
        "        r'(.*?\\?)\\s*(Yes|No)',              # 以问号结尾的问题 + 答案 格式\n",
        "    ]\n",
        "\n",
        "def setup_logging() -> logging.Logger:\n",
        "    \"\"\"\n",
        "    设置日志系统配置\n",
        "\n",
        "    配置日志记录器，使其能够同时将日志信息输出到文件和控制台。\n",
        "    这样既能实时查看处理进度，又能保存完整的处理记录。\n",
        "\n",
        "    返回:\n",
        "        logging.Logger: 配置好的日志记录器实例\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,  # 设置日志级别为INFO\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',  # 日志格式\n",
        "        handlers=[\n",
        "            logging.FileHandler(DatasetConfig.LOG_FILE),  # 文件输出\n",
        "            logging.StreamHandler()  # 控制台输出\n",
        "        ]\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# 2. 数据加载模块\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetLoader:\n",
        "    \"\"\"\n",
        "    数据集加载器 - 负责从HuggingFace加载和预处理数据集\n",
        "\n",
        "    这个类封装了数据集加载的所有操作，包括：\n",
        "    - 从HuggingFace datasets库加载指定数据集\n",
        "    - 验证数据集结构的完整性\n",
        "    - 转换为pandas DataFrame便于后续处理\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        初始化数据加载器\n",
        "\n",
        "        参数:\n",
        "            logger: 日志记录器实例\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "\n",
        "    def load_finance_dataset(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        加载AdaptLLM/finance-tasks数据集的headline子集\n",
        "\n",
        "        这个方法使用HuggingFace的datasets库来加载指定的数据集，\n",
        "        然后将其转换为pandas DataFrame格式，便于后续的数据处理操作。\n",
        "\n",
        "        返回:\n",
        "            pd.DataFrame: 加载后的数据集DataFrame\n",
        "\n",
        "        异常:\n",
        "            Exception: 当数据集加载失败时抛出异常\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"开始加载数据集: {DatasetConfig.DATASET_NAME}\")\n",
        "            self.logger.info(f\"目标子集: {DatasetConfig.SUBSET_NAME}\")\n",
        "\n",
        "            # 使用HuggingFace datasets库加载数据集\n",
        "            # split='train' 表示加载训练集部分\n",
        "            dataset = load_dataset(\n",
        "                DatasetConfig.DATASET_NAME,\n",
        "                DatasetConfig.SUBSET_NAME,\n",
        "                split='train'\n",
        "            )\n",
        "\n",
        "            # 将HuggingFace Dataset对象转换为pandas DataFrame\n",
        "            # 这样可以利用pandas强大的数据处理功能\n",
        "            df = dataset.to_pandas()\n",
        "\n",
        "            self.logger.info(f\"数据集加载成功! 数据形状: {df.shape}\")\n",
        "            self.logger.info(f\"数据列名: {list(df.columns)}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"数据集加载失败: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def validate_dataset_structure(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        验证数据集结构是否符合预期\n",
        "\n",
        "        检查数据集是否包含必需的列，以及数据的基本完整性。\n",
        "        这是数据处理前的重要验证步骤，确保后续处理不会出错。\n",
        "\n",
        "        参数:\n",
        "            df (pd.DataFrame): 要验证的数据集\n",
        "\n",
        "        返回:\n",
        "            bool: 验证通过返回True\n",
        "\n",
        "        异常:\n",
        "            ValueError: 当数据集结构不符合要求时抛出异常\n",
        "        \"\"\"\n",
        "        # 定义必需的列名\n",
        "        required_columns = ['input']\n",
        "\n",
        "        # 检查是否包含所有必需的列\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "            raise ValueError(f\"缺少必需的列: {missing_cols}\")\n",
        "\n",
        "        # 检查input列是否完全为空\n",
        "        if df['input'].isnull().all():\n",
        "            raise ValueError(\"input列完全为空，无法处理\")\n",
        "\n",
        "        self.logger.info(\"数据集结构验证通过\")\n",
        "        return True\n",
        "\n",
        "# ============================================================================\n",
        "# 3. 问答提取模块\n",
        "# ============================================================================\n",
        "\n",
        "class QuestionAnswerExtractor:\n",
        "    \"\"\"\n",
        "    问答对提取器 - 核心处理模块\n",
        "\n",
        "    这是整个系统的核心组件，负责从原始文本中提取问答对。\n",
        "    主要功能包括：\n",
        "    - 使用多种正则表达式模式匹配问答对\n",
        "    - 清理和标准化文本内容\n",
        "    - 验证提取结果的质量\n",
        "    - 去除重复的问答对\n",
        "    - 生成结构化的输出记录\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        初始化问答提取器\n",
        "\n",
        "        参数:\n",
        "            logger: 日志记录器实例\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        # 统计信息字典，用于跟踪处理效果\n",
        "        self.extraction_stats = {\n",
        "            'total_entries': 0,        # 总处理条目数\n",
        "            'successful_extractions': 0,  # 成功提取的条目数\n",
        "            'failed_extractions': 0,   # 提取失败的条目数\n",
        "            'total_qa_pairs': 0        # 总问答对数量\n",
        "        }\n",
        "\n",
        "    def extract_qa_pairs(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        从给定文本中提取问答对\n",
        "\n",
        "        这是核心提取方法，使用多种正则表达式模式来识别文本中的问答对。\n",
        "        方法会尝试所有预定义的模式，确保最大程度地提取有效信息。\n",
        "\n",
        "        参数:\n",
        "            text (str): 包含问答对的输入文本\n",
        "\n",
        "        返回:\n",
        "            List[Dict[str, str]]: 提取到的问答对列表\n",
        "        \"\"\"\n",
        "        # 处理空值情况\n",
        "        if not text or pd.isna(text):\n",
        "            return []\n",
        "\n",
        "        qa_pairs = []\n",
        "        text_cleaned = self._clean_text(text)  # 清理文本\n",
        "\n",
        "        # 尝试使用每个正则表达式模式进行匹配\n",
        "        for pattern in DatasetConfig.QA_PATTERNS:\n",
        "            matches = re.findall(pattern, text_cleaned, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "            # 处理每个匹配结果\n",
        "            for match in matches:\n",
        "                if len(match) == 2:  # 确保匹配到问题和答案两部分\n",
        "                    question, answer = match\n",
        "                    question = self._clean_question(question.strip())\n",
        "                    answer = answer.strip()\n",
        "\n",
        "                    # 验证问答对的有效性\n",
        "                    if self._validate_qa_pair(question, answer):\n",
        "                        qa_pairs.append({\n",
        "                            'question': question,\n",
        "                            'answer': answer\n",
        "                        })\n",
        "\n",
        "        # 去除重复的问答对，但保持原有顺序\n",
        "        qa_pairs = self._remove_duplicate_pairs(qa_pairs)\n",
        "\n",
        "        return qa_pairs\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        清理和标准化输入文本\n",
        "\n",
        "        对原始文本进行预处理，包括：\n",
        "        - 规范化空白字符和换行符\n",
        "        - 统一问答标记格式\n",
        "        - 去除多余的符号\n",
        "\n",
        "        参数:\n",
        "            text (str): 原始输入文本\n",
        "\n",
        "        返回:\n",
        "            str: 清理后的文本\n",
        "        \"\"\"\n",
        "        # 规范化空白字符，将多个连续空格替换为单个空格\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # 标准化问题标记格式\n",
        "        text = re.sub(r'Q\\d*[\\.:]\\s*', 'Q: ', text)        # Q1: -> Q:\n",
        "        text = re.sub(r'Question\\d*[\\.:]\\s*', 'Question: ', text)  # Question1: -> Question:\n",
        "        text = re.sub(r'A\\d*[\\.:]\\s*', 'A: ', text)        # A1: -> A:\n",
        "        text = re.sub(r'Answer\\d*[\\.:]\\s*', 'Answer: ', text)      # Answer1: -> Answer:\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _clean_question(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        清理和格式化问题文本\n",
        "\n",
        "        对提取出的问题文本进行进一步处理：\n",
        "        - 去除编号和项目符号\n",
        "        - 为疑问句添加问号（如果缺少）\n",
        "        - 统一格式\n",
        "\n",
        "        参数:\n",
        "            question (str): 原始问题文本\n",
        "\n",
        "        返回:\n",
        "            str: 清理后的问题文本\n",
        "        \"\"\"\n",
        "        # 去除开头的数字编号和项目符号\n",
        "        question = re.sub(r'^\\d+[\\.\\)]\\s*', '', question)  # 1. 或 1)\n",
        "        question = re.sub(r'^[•\\-\\*]\\s*', '', question)    # • - *\n",
        "\n",
        "        # 如果是疑问句但没有问号，则添加问号\n",
        "        # 通过检查是否包含疑问词来判断\n",
        "        question_words = ['is', 'are', 'was', 'were', 'do', 'does', 'did',\n",
        "                         'can', 'could', 'will', 'would', 'should', 'has', 'have']\n",
        "\n",
        "        if (not question.endswith('?') and\n",
        "            any(word in question.lower() for word in question_words)):\n",
        "            question += '?'\n",
        "\n",
        "        return question.strip()\n",
        "\n",
        "    def _validate_qa_pair(self, question: str, answer: str) -> bool:\n",
        "        \"\"\"\n",
        "        验证提取的问答对是否有效\n",
        "\n",
        "        对提取的问答对进行质量检查，包括：\n",
        "        - 检查长度是否合理\n",
        "        - 验证答案是否为Yes/No\n",
        "        - 避免无意义的问题\n",
        "\n",
        "        参数:\n",
        "            question (str): 问题文本\n",
        "            answer (str): 答案文本\n",
        "\n",
        "        返回:\n",
        "            bool: 有效返回True，否则返回False\n",
        "        \"\"\"\n",
        "        # 检查最小长度要求\n",
        "        if len(question.strip()) < 5 or len(answer.strip()) < 2:\n",
        "            return False\n",
        "\n",
        "        # 检查答案是否为Yes或No\n",
        "        if answer.lower() not in ['yes', 'no']:\n",
        "            return False\n",
        "\n",
        "        # 避免无意义或重复的问题\n",
        "        if question.lower() in ['yes', 'no', 'question', 'answer']:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _remove_duplicate_pairs(self, qa_pairs: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        去除重复的问答对\n",
        "\n",
        "        使用哈希值来识别重复的问答对，确保结果中不包含重复内容，\n",
        "        同时保持原有的顺序。\n",
        "\n",
        "        参数:\n",
        "            qa_pairs (List[Dict[str, str]]): 问答对列表\n",
        "\n",
        "        返回:\n",
        "            List[Dict[str, str]]: 去重后的问答对列表\n",
        "        \"\"\"\n",
        "        seen = set()  # 用于存储已见过的哈希值\n",
        "        unique_pairs = []\n",
        "\n",
        "        for pair in qa_pairs:\n",
        "            # 为问答对创建唯一的哈希标识\n",
        "            pair_content = f\"{pair['question'].lower()}-{pair['answer'].lower()}\"\n",
        "            pair_hash = hashlib.md5(pair_content.encode()).hexdigest()\n",
        "\n",
        "            # 如果是新的问答对，则添加到结果中\n",
        "            if pair_hash not in seen:\n",
        "                seen.add(pair_hash)\n",
        "                unique_pairs.append(pair)\n",
        "\n",
        "        return unique_pairs\n",
        "\n",
        "    def process_dataset(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        处理整个数据集，提取所有问答对\n",
        "\n",
        "        这是数据集级别的处理方法，遍历数据集中的每一行，\n",
        "        提取其中的问答对，并生成结构化的记录。\n",
        "\n",
        "        参数:\n",
        "            df (pd.DataFrame): 输入数据集\n",
        "\n",
        "        返回:\n",
        "            List[Dict[str, Any]]: 结构化的问答记录列表\n",
        "        \"\"\"\n",
        "        all_qa_records = []\n",
        "        self.extraction_stats['total_entries'] = len(df)\n",
        "\n",
        "        self.logger.info(f\"开始处理 {len(df)} 条数据集条目...\")\n",
        "\n",
        "        # 遍历数据集的每一行\n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                input_text = row['input']\n",
        "                qa_pairs = self.extract_qa_pairs(input_text)\n",
        "\n",
        "                if qa_pairs:\n",
        "                    self.extraction_stats['successful_extractions'] += 1\n",
        "\n",
        "                    # 为每个问答对创建结构化记录\n",
        "                    for qa_idx, qa_pair in enumerate(qa_pairs):\n",
        "                        record = self._create_qa_record(\n",
        "                            original_idx=idx,\n",
        "                            qa_idx=qa_idx,\n",
        "                            question=qa_pair['question'],\n",
        "                            answer=qa_pair['answer'],\n",
        "                            source_row=row\n",
        "                        )\n",
        "                        all_qa_records.append(record)\n",
        "                        self.extraction_stats['total_qa_pairs'] += 1\n",
        "                else:\n",
        "                    self.extraction_stats['failed_extractions'] += 1\n",
        "                    self.logger.debug(f\"在条目 {idx} 中未找到问答对\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.extraction_stats['failed_extractions'] += 1\n",
        "                self.logger.error(f\"处理条目 {idx} 时出错: {str(e)}\")\n",
        "\n",
        "        self.logger.info(f\"提取完成，共找到 {len(all_qa_records)} 个问答对\")\n",
        "        return all_qa_records\n",
        "\n",
        "    def _create_qa_record(self, original_idx: int, qa_idx: int, question: str,\n",
        "                         answer: str, source_row: pd.Series) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        创建结构化的问答记录\n",
        "\n",
        "        将提取的问答对转换为标准化的JSON记录格式，\n",
        "        包含问题、答案以及相关的元数据信息。\n",
        "\n",
        "        参数:\n",
        "            original_idx (int): 原始数据集中的索引\n",
        "            qa_idx (int): 该条目中问答对的索引\n",
        "            question (str): 问题文本\n",
        "            answer (str): 答案文本\n",
        "            source_row (pd.Series): 原始数据行\n",
        "\n",
        "        返回:\n",
        "            Dict[str, Any]: 结构化的问答记录\n",
        "        \"\"\"\n",
        "        # 生成唯一标识符\n",
        "        unique_id = f\"finance_headline_{original_idx:06d}_{qa_idx:03d}\"\n",
        "\n",
        "        # 创建基础记录结构\n",
        "        record = {\n",
        "            \"id\": unique_id,\n",
        "            \"Question\": question,\n",
        "            \"Answer\": answer,\n",
        "            \"metadata\": {\n",
        "                \"source_dataset\": DatasetConfig.DATASET_NAME,\n",
        "                \"subset\": DatasetConfig.SUBSET_NAME,\n",
        "                \"original_index\": original_idx,\n",
        "                \"qa_pair_index\": qa_idx,\n",
        "                \"extraction_timestamp\": datetime.now().isoformat(),\n",
        "                \"tags\": self._generate_tags(question, answer)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 添加源数据的其他字段（如果存在）\n",
        "        for col in source_row.index:\n",
        "            if col not in ['input'] and pd.notna(source_row[col]):\n",
        "                record[\"metadata\"][f\"source_{col}\"] = source_row[col]\n",
        "\n",
        "        return record\n",
        "\n",
        "    def _generate_tags(self, question: str, answer: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        为问答对生成相关标签\n",
        "\n",
        "        基于问题内容和答案，自动生成相关的标签，\n",
        "        这些标签有助于后续的分类和检索。\n",
        "\n",
        "        参数:\n",
        "            question (str): 问题文本\n",
        "            answer (str): 答案文本\n",
        "\n",
        "        返回:\n",
        "            List[str]: 相关标签列表\n",
        "        \"\"\"\n",
        "        # 基础标签\n",
        "        tags = [\"finance\", \"headline\", \"binary_classification\"]\n",
        "\n",
        "        # 基于答案的标签\n",
        "        tags.append(f\"answer_{answer.lower()}\")\n",
        "\n",
        "        # 基于内容的标签分析\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # 价格相关\n",
        "        if any(word in question_lower for word in ['price', 'cost', 'dollar', '$', 'money']):\n",
        "            tags.append(\"pricing\")\n",
        "\n",
        "        # 公司相关\n",
        "        if any(word in question_lower for word in ['company', 'corporation', 'firm', 'business']):\n",
        "            tags.append(\"corporate\")\n",
        "\n",
        "        # 市场相关\n",
        "        if any(word in question_lower for word in ['market', 'stock', 'share', 'trading']):\n",
        "            tags.append(\"market\")\n",
        "\n",
        "        # 财务表现相关\n",
        "        if any(word in question_lower for word in ['revenue', 'profit', 'earnings', 'income']):\n",
        "            tags.append(\"financial_performance\")\n",
        "\n",
        "        # 趋势分析相关\n",
        "        if any(word in question_lower for word in ['increase', 'decrease', 'rise', 'fall', 'growth']):\n",
        "            tags.append(\"trend_analysis\")\n",
        "\n",
        "        return tags\n",
        "\n",
        "    def get_extraction_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        获取提取过程的详细统计信息\n",
        "\n",
        "        计算并返回提取过程的各种统计指标，包括成功率、\n",
        "        平均问答对数量等，用于评估处理效果。\n",
        "\n",
        "        返回:\n",
        "            Dict[str, Any]: 提取统计信息\n",
        "        \"\"\"\n",
        "        # 计算成功率\n",
        "        success_rate = (self.extraction_stats['successful_extractions'] /\n",
        "                       max(1, self.extraction_stats['total_entries'])) * 100\n",
        "\n",
        "        # 计算平均问答对数量\n",
        "        avg_pairs_per_entry = (self.extraction_stats['total_qa_pairs'] /\n",
        "                              max(1, self.extraction_stats['successful_extractions']))\n",
        "\n",
        "        return {\n",
        "            **self.extraction_stats,\n",
        "            'success_rate_percent': round(success_rate, 2),\n",
        "            'average_pairs_per_successful_entry': round(avg_pairs_per_entry, 2)\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# 4. 输出和序列化模块\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetSerializer:\n",
        "    \"\"\"\n",
        "    数据集序列化器 - 负责保存和格式化处理结果\n",
        "\n",
        "    这个类处理所有与数据输出相关的操作：\n",
        "    - 将问答记录保存为JSON文件\n",
        "    - 生成数据格式的文档说明\n",
        "    - 确保输出格式的一致性和可读性\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        初始化序列化器\n",
        "\n",
        "        参数:\n",
        "            logger: 日志记录器实例\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "\n",
        "    def save_qa_records(self, qa_records: List[Dict[str, Any]],\n",
        "                       output_file: str = None) -> str:\n",
        "        \"\"\"\n",
        "        保存问答记录到JSON文件\n",
        "\n",
        "        将处理好的问答记录以JSON格式保存到文件，\n",
        "        使用UTF-8编码确保中文等特殊字符正确显示。\n",
        "\n",
        "        参数:\n",
        "            qa_records (List[Dict[str, Any]]): 问答记录列表\n",
        "            output_file (str, optional): 输出文件路径\n",
        "\n",
        "        返回:\n",
        "            str: 保存文件的路径\n",
        "        \"\"\"\n",
        "        if output_file is None:\n",
        "            output_file = DatasetConfig.OUTPUT_FILE\n",
        "\n",
        "        try:\n",
        "            # 以UTF-8编码保存JSON文件，确保格式美观\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_records, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            self.logger.info(f\"成功保存 {len(qa_records)} 条记录到 {output_file}\")\n",
        "            return output_file\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"保存记录失败: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_schema_documentation(self, qa_records: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        生成输出格式的文档说明\n",
        "\n",
        "        创建详细的数据格式文档，包括字段说明、数据类型、\n",
        "        示例记录等，便于理解和使用输出数据。\n",
        "\n",
        "        参数:\n",
        "            qa_records (List[Dict[str, Any]]): 样本记录\n",
        "\n",
        "        返回:\n",
        "            Dict[str, Any]: 格式文档\n",
        "        \"\"\"\n",
        "        schema = {\n",
        "            \"format_version\": \"1.0\",\n",
        "            \"description\": \"从AdaptLLM/finance-tasks headline子集提取的结构化问答对\",\n",
        "            \"record_structure\": {\n",
        "                \"id\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"问答对的唯一标识符\",\n",
        "                    \"pattern\": \"finance_headline_XXXXXX_XXX\",\n",
        "                    \"example\": \"finance_headline_000001_001\"\n",
        "                },\n",
        "                \"Question\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"提取的问题文本\"\n",
        "                },\n",
        "                \"Answer\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"问题的答案（Yes或No）\",\n",
        "                    \"enum\": [\"Yes\", \"No\"]\n",
        "                },\n",
        "                \"metadata\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"description\": \"记录的附加元数据信息\",\n",
        "                    \"properties\": {\n",
        "                        \"source_dataset\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"源数据集名称\"\n",
        "                        },\n",
        "                        \"subset\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"数据集子集名称\"\n",
        "                        },\n",
        "                        \"original_index\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"在原始数据集中的索引\"\n",
        "                        },\n",
        "                        \"qa_pair_index\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"在该条目中的问答对索引\"\n",
        "                        },\n",
        "                        \"extraction_timestamp\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"format\": \"ISO8601\",\n",
        "                            \"description\": \"提取时间戳\"\n",
        "                        },\n",
        "                        \"tags\": {\n",
        "                            \"type\": \"array\",\n",
        "                            \"items\": {\"type\": \"string\"},\n",
        "                            \"description\": \"自动生成的内容标签\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"sample_record\": qa_records[0] if qa_records else None,\n",
        "            \"total_records\": len(qa_records),\n",
        "            \"usage_notes\": [\n",
        "                \"每个记录包含一个问题和对应的Yes/No答案\",\n",
        "                \"id字段确保每个问答对的唯一性\",\n",
        "                \"metadata包含丰富的上下文信息\",\n",
        "                \"tags字段便于分类和检索\",\n",
        "                \"所有文本均经过清理和标准化处理\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return schema\n",
        "\n",
        "# ============================================================================\n",
        "# 5. 性能监控模块\n",
        "# ============================================================================\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"\n",
        "    性能监控器 - 跟踪和报告处理性能\n",
        "\n",
        "    监控整个处理流程的性能指标，包括：\n",
        "    - 总处理时间\n",
        "    - 各个阶段的耗时\n",
        "    - 处理速度统计\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        初始化性能监控器\n",
        "\n",
        "        参数:\n",
        "            logger: 日志记录器实例\n",
        "        \"\"\"\n",
        "        self.logger = logger\n",
        "        self.start_time = None      # 开始时间\n",
        "        self.end_time = None        # 结束时间\n",
        "        self.checkpoints = {}       # 检查点时间记录\n",
        "\n",
        "    def start_monitoring(self):\n",
        "        \"\"\"开始性能监控\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.logger.info(\"开始性能监控\")\n",
        "\n",
        "    def checkpoint(self, name: str):\n",
        "        \"\"\"\n",
        "        记录性能检查点\n",
        "\n",
        "        在处理过程的关键节点记录时间，\n",
        "        用于分析各个阶段的耗时情况。\n",
        "\n",
        "        参数:\n",
        "            name (str): 检查点名称\n",
        "        \"\"\"\n",
        "        if self.start_time is None:\n",
        "            self.start_monitoring()\n",
        "\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        self.checkpoints[name] = elapsed_time\n",
        "        self.logger.info(f\"检查点 '{name}': {elapsed_time:.2f} 秒\")\n",
        "\n",
        "    def finish_monitoring(self) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        结束监控并返回性能指标\n",
        "\n",
        "        计算总处理时间和各阶段用时，\n",
        "        生成完整的性能报告。\n",
        "\n",
        "        返回:\n",
        "            Dict[str, float]: 性能指标字典\n",
        "        \"\"\"\n",
        "        self.end_time = time.time()\n",
        "        total_time = self.end_time - self.start_time\n",
        "\n",
        "        # 构建性能指标字典\n",
        "        metrics = {\n",
        "            'total_processing_time_seconds': round(total_time, 2),\n",
        "            'total_processing_time_minutes': round(total_time / 60, 2),\n",
        "            # 添加各检查点的用时\n",
        "            **{f\"{name}_seconds\": round(time_val, 2)\n",
        "               for name, time_val in self.checkpoints.items()}\n",
        "        }\n",
        "\n",
        "        self.logger.info(f\"总处理时间: {metrics['total_processing_time_seconds']} 秒\")\n",
        "        return metrics\n",
        "\n",
        "# ============================================================================\n",
        "# 6. 主要协调模块\n",
        "# ============================================================================\n",
        "\n",
        "class FinanceDatasetProcessor:\n",
        "    \"\"\"\n",
        "    金融数据集处理器 - 主要协调类\n",
        "\n",
        "    这是整个系统的核心协调器，负责：\n",
        "    - 协调各个模块的工作\n",
        "    - 控制整个处理流程\n",
        "    - 处理异常和错误\n",
        "    - 生成最终报告\n",
        "\n",
        "    使用示例:\n",
        "        processor = FinanceDatasetProcessor()\n",
        "        results = processor.process_dataset()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        初始化处理器，创建所有必要的组件实例\n",
        "        \"\"\"\n",
        "        self.logger = setup_logging()                           # 日志记录器\n",
        "        self.loader = DatasetLoader(self.logger)                # 数据加载器\n",
        "        self.extractor = QuestionAnswerExtractor(self.logger)   # 问答提取器\n",
        "        self.serializer = DatasetSerializer(self.logger)       # 数据序列化器\n",
        "        self.monitor = PerformanceMonitor(self.logger)          # 性能监控器\n",
        "\n",
        "    def process_dataset(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        执行完整的数据集处理流程\n",
        "\n",
        "        这是整个系统的主要入口方法，按顺序执行以下步骤：\n",
        "        1. 加载数据集\n",
        "        2. 提取问答对\n",
        "        3. 保存结果\n",
        "        4. 生成报告\n",
        "\n",
        "        返回:\n",
        "            Dict[str, Any]: 包含处理结果和统计信息的字典\n",
        "        \"\"\"\n",
        "        self.logger.info(\"开始执行金融数据集处理流程\")\n",
        "        self.monitor.start_monitoring()\n",
        "\n",
        "        try:\n",
        "            # 步骤1: 加载数据集\n",
        "            self.logger.info(\"步骤1: 正在加载数据集...\")\n",
        "            df = self.loader.load_finance_dataset()\n",
        "            self.loader.validate_dataset_structure(df)\n",
        "            self.monitor.checkpoint(\"数据集加载完成\")\n",
        "\n",
        "            # 步骤2: 提取问答对\n",
        "            self.logger.info(\"步骤2: 正在提取问答对...\")\n",
        "            qa_records = self.extractor.process_dataset(df)\n",
        "            self.monitor.checkpoint(\"问答提取完成\")\n",
        "\n",
        "            # 步骤3: 保存结果\n",
        "            self.logger.info(\"步骤3: 正在保存处理结果...\")\n",
        "            output_file = self.serializer.save_qa_records(qa_records)\n",
        "            self.monitor.checkpoint(\"数据保存完成\")\n",
        "\n",
        "            # 步骤4: 生成文档和统计信息\n",
        "            self.logger.info(\"步骤4: 正在生成文档和统计...\")\n",
        "            schema_doc = self.serializer.generate_schema_documentation(qa_records)\n",
        "            extraction_stats = self.extractor.get_extraction_statistics()\n",
        "            performance_metrics = self.monitor.finish_monitoring()\n",
        "\n",
        "            # 编译最终结果\n",
        "            results = {\n",
        "                \"processing_summary\": {\n",
        "                    \"status\": \"已完成\",\n",
        "                    \"output_file\": output_file,\n",
        "                    \"total_qa_pairs\": len(qa_records)\n",
        "                },\n",
        "                \"extraction_statistics\": extraction_stats,\n",
        "                \"performance_metrics\": performance_metrics,\n",
        "                \"schema_documentation\": schema_doc\n",
        "            }\n",
        "\n",
        "            self.logger.info(\"数据集处理成功完成!\")\n",
        "            self._print_summary(results)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"数据集处理失败: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _print_summary(self, results: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        打印格式化的处理结果摘要\n",
        "\n",
        "        在控制台输出美观的处理结果摘要，包括：\n",
        "        - 处理状态\n",
        "        - 统计信息\n",
        "        - 性能指标\n",
        "\n",
        "        参数:\n",
        "            results (Dict[str, Any]): 处理结果字典\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"🏦 金融数据集处理结果摘要\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # 处理摘要\n",
        "        summary = results[\"processing_summary\"]\n",
        "        print(f\"📊 处理状态: {summary['status']}\")\n",
        "        print(f\"📁 输出文件: {summary['output_file']}\")\n",
        "        print(f\"💬 问答对总数: {summary['total_qa_pairs']:,}\")\n",
        "\n",
        "        # 提取统计\n",
        "        stats = results[\"extraction_statistics\"]\n",
        "        print(f\"\\n📈 提取统计信息:\")\n",
        "        print(f\"  • 处理条目总数: {stats['total_entries']:,}\")\n",
        "        print(f\"  • 成功提取条目: {stats['successful_extractions']:,}\")\n",
        "        print(f\"  • 提取失败条目: {stats['failed_extractions']:,}\")\n",
        "        print(f\"  • 成功率: {stats['success_rate_percent']:.1f}%\")\n",
        "        print(f\"  • 平均每条目问答对数: {stats['average_pairs_per_successful_entry']:.1f}\")\n",
        "\n",
        "        # 性能指标\n",
        "        perf = results[\"performance_metrics\"]\n",
        "        print(f\"\\n⏱️  性能指标:\")\n",
        "        print(f\"  • 总处理时间: {perf['total_processing_time_seconds']:.2f} 秒\")\n",
        "        print(f\"  • 总处理时间: {perf['total_processing_time_minutes']:.2f} 分钟\")\n",
        "\n",
        "        # 显示各阶段用时\n",
        "        if '数据集加载完成_seconds' in perf:\n",
        "            print(f\"  • 数据加载用时: {perf['数据集加载完成_seconds']:.2f} 秒\")\n",
        "        if '问答提取完成_seconds' in perf:\n",
        "            extract_time = perf['问答提取完成_seconds'] - perf.get('数据集加载完成_seconds', 0)\n",
        "            print(f\"  • 问答提取用时: {extract_time:.2f} 秒\")\n",
        "        if '数据保存完成_seconds' in perf:\n",
        "            save_time = perf['数据保存完成_seconds'] - perf.get('问答提取完成_seconds', 0)\n",
        "            print(f\"  • 数据保存用时: {save_time:.2f} 秒\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"✅ 处理完成! 可以查看输出文件获取详细结果。\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. 主运行函数\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    主运行函数 - 程序入口点\n",
        "\n",
        "    这是整个程序的入口函数，负责：\n",
        "    1. 初始化处理器\n",
        "    2. 执行处理流程\n",
        "    3. 保存详细结果\n",
        "    4. 处理异常情况\n",
        "\n",
        "    可以直接运行脚本或导入后调用此函数。\n",
        "\n",
        "    返回:\n",
        "        Dict[str, Any]: 处理结果，如果处理失败则返回None\n",
        "    \"\"\"\n",
        "    print(\"🚀 开始启动金融数据集处理器...\")\n",
        "    print(\"📋 任务: 处理AdaptLLM/finance-tasks数据集的headline子集\")\n",
        "    print(\"🎯 目标: 提取问答对并转换为结构化JSON格式\\n\")\n",
        "\n",
        "    try:\n",
        "        # 初始化并运行处理器\n",
        "        processor = FinanceDatasetProcessor()\n",
        "        results = processor.process_dataset()\n",
        "\n",
        "        # 保存详细的处理结果供后续参考\n",
        "        results_file = \"processing_results.json\"\n",
        "        with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"📄 详细处理结果已保存到: {results_file}\")\n",
        "        print(\"🎉 所有任务已成功完成!\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n⚠️  用户中断了处理过程\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ 处理过程中发生错误: {str(e)}\")\n",
        "        print(\"💡 请检查日志文件获取详细错误信息\")\n",
        "        raise\n",
        "\n",
        "# ============================================================================\n",
        "# 8. 辅助函数 - 单独测试各模块\n",
        "# ============================================================================\n",
        "\n",
        "def test_data_loading():\n",
        "    \"\"\"\n",
        "    测试数据加载功能\n",
        "    用于单独测试数据集加载是否正常工作\n",
        "    \"\"\"\n",
        "    print(\"🧪 测试数据加载模块...\")\n",
        "    logger = setup_logging()\n",
        "    loader = DatasetLoader(logger)\n",
        "\n",
        "    try:\n",
        "        df = loader.load_finance_dataset()\n",
        "        loader.validate_dataset_structure(df)\n",
        "        print(f\"✅ 数据加载测试成功! 数据集形状: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 数据加载测试失败: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def test_qa_extraction(sample_text: str = None):\n",
        "    \"\"\"\n",
        "    测试问答提取功能\n",
        "    用于单独测试问答对提取是否正常工作\n",
        "\n",
        "    参数:\n",
        "        sample_text (str): 测试用的样本文本\n",
        "    \"\"\"\n",
        "    print(\"🧪 测试问答提取模块...\")\n",
        "    logger = setup_logging()\n",
        "    extractor = QuestionAnswerExtractor(logger)\n",
        "\n",
        "    # 使用示例文本进行测试\n",
        "    if sample_text is None:\n",
        "        sample_text = \"\"\"\n",
        "        Q: Is the company's revenue over $1 million? A: Yes\n",
        "        Question: Did the stock price increase? Answer: No\n",
        "        1. Was there a merger announcement? Yes\n",
        "        Does the earnings report show profit? No\n",
        "        \"\"\"\n",
        "\n",
        "    try:\n",
        "        qa_pairs = extractor.extract_qa_pairs(sample_text)\n",
        "        print(f\"✅ 问答提取测试成功! 提取到 {len(qa_pairs)} 个问答对:\")\n",
        "        for i, pair in enumerate(qa_pairs, 1):\n",
        "            print(f\"  {i}. Q: {pair['question']}\")\n",
        "            print(f\"     A: {pair['answer']}\")\n",
        "        return qa_pairs\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 问答提取测试失败: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def show_usage_examples():\n",
        "    \"\"\"\n",
        "    显示使用示例和说明\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "🔧 使用方法和示例:\n",
        "\n",
        "1. 直接运行脚本:\n",
        "   python finance_dataset_processor.py\n",
        "\n",
        "2. 在Python代码中使用:\n",
        "   from finance_dataset_processor import main, FinanceDatasetProcessor\n",
        "\n",
        "   # 方法1: 使用main函数\n",
        "   results = main()\n",
        "\n",
        "   # 方法2: 直接使用处理器类\n",
        "   processor = FinanceDatasetProcessor()\n",
        "   results = processor.process_dataset()\n",
        "\n",
        "3. 测试特定模块:\n",
        "   from finance_dataset_processor import test_data_loading, test_qa_extraction\n",
        "\n",
        "   # 测试数据加载\n",
        "   df = test_data_loading()\n",
        "\n",
        "   # 测试问答提取\n",
        "   qa_pairs = test_qa_extraction()\n",
        "\n",
        "4. 输出文件说明:\n",
        "   - finance_headline_qa_pairs.json: 主要的问答对数据\n",
        "   - processing_results.json: 详细的处理结果和统计\n",
        "   - processing.log: 完整的处理日志\n",
        "\n",
        "📊 输出JSON格式示例:\n",
        "{\n",
        "  \"id\": \"finance_headline_000001_001\",\n",
        "  \"Question\": \"Does the company's revenue exceed $1 billion?\",\n",
        "  \"Answer\": \"Yes\",\n",
        "  \"metadata\": {\n",
        "    \"source_dataset\": \"AdaptLLM/finance-tasks\",\n",
        "    \"subset\": \"headline\",\n",
        "    \"original_index\": 1,\n",
        "    \"qa_pair_index\": 1,\n",
        "    \"extraction_timestamp\": \"2025-05-24T10:30:45.123456\",\n",
        "    \"tags\": [\"finance\", \"headline\", \"binary_classification\", \"answer_yes\"]\n",
        "  }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. 程序入口点\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 检查命令行参数\n",
        "    import sys\n",
        "\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "-YRY45GmzbZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634,
          "referenced_widgets": [
            "e5772fa069884d7089f0cf5e70c6fbd1",
            "9427b01ff02240c0ae71d690f2e87fb1",
            "eeeaa904083a46fb95f217814f9f329e",
            "6d46ce74724c4d869c65d55a7a8bbbd9",
            "1c24edc4f85342a88bfcc184817550a7",
            "5b8b6cd8531a49519a7342ba3fddd574",
            "489b0e49949d4dc697427c58d684d2f8",
            "ba952c8e6b7f4c6eb886006d36b15bab",
            "17948f0e9cbd45d6b3e7c0c6ef192756",
            "d5b6aa82caaf4c2fbfab336171ccd29d",
            "f361658c6786488c93a91fa62be002d7"
          ]
        },
        "outputId": "19622623-5147-428d-9e8f-631849d95d0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 开始启动金融数据集处理器...\n",
            "📋 任务: 处理AdaptLLM/finance-tasks数据集的headline子集\n",
            "🎯 目标: 提取问答对并转换为结构化JSON格式\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5772fa069884d7089f0cf5e70c6fbd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:数据集加载失败: Invalid pattern: '**' can only be an entire path component\n",
            "ERROR:__main__:数据集处理失败: Invalid pattern: '**' can only be an entire path component\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ 处理过程中发生错误: Invalid pattern: '**' can only be an entire path component\n",
            "💡 请检查日志文件获取详细错误信息\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid pattern: '**' can only be an entire path component",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;31m# 初始化并运行处理器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinanceDatasetProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;31m# 保存详细的处理结果供后续参考\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# 步骤1: 加载数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"步骤1: 正在加载数据集...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_finance_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"数据集加载完成\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6d248eee7cec>\u001b[0m in \u001b[0;36mload_finance_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# 使用HuggingFace datasets库加载数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# split='train' 表示加载训练集部分\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             dataset = load_dataset(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mDatasetConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mDatasetConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUBSET_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2113\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     ) from None\n\u001b[0;32m-> 1495\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m                     \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m                     \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1480\u001b[0m         except (\n\u001b[1;32m   1481\u001b[0m             \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         )\n\u001b[1;32m   1036\u001b[0m         data_files = DataFilesDict.from_patterns(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mget_data_patterns\u001b[0;34m(base_path, download_config)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolve_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_data_files_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The directory at {base_path} doesn't contain any data files\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_path_and_storage_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fs_token_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0mfs_base_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_marker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mfs_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"expand_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     def find(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mallpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mends_with_sep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/utils.py\u001b[0m in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"**\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                 \u001b[0;34m\"Invalid pattern: '**' can only be an entire path component\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5772fa069884d7089f0cf5e70c6fbd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9427b01ff02240c0ae71d690f2e87fb1",
              "IPY_MODEL_eeeaa904083a46fb95f217814f9f329e",
              "IPY_MODEL_6d46ce74724c4d869c65d55a7a8bbbd9"
            ],
            "layout": "IPY_MODEL_1c24edc4f85342a88bfcc184817550a7"
          }
        },
        "9427b01ff02240c0ae71d690f2e87fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b8b6cd8531a49519a7342ba3fddd574",
            "placeholder": "​",
            "style": "IPY_MODEL_489b0e49949d4dc697427c58d684d2f8",
            "value": "Downloading readme: 100%"
          }
        },
        "eeeaa904083a46fb95f217814f9f329e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba952c8e6b7f4c6eb886006d36b15bab",
            "max": 8230,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17948f0e9cbd45d6b3e7c0c6ef192756",
            "value": 8230
          }
        },
        "6d46ce74724c4d869c65d55a7a8bbbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5b6aa82caaf4c2fbfab336171ccd29d",
            "placeholder": "​",
            "style": "IPY_MODEL_f361658c6786488c93a91fa62be002d7",
            "value": " 8.23k/8.23k [00:00&lt;00:00, 558kB/s]"
          }
        },
        "1c24edc4f85342a88bfcc184817550a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b8b6cd8531a49519a7342ba3fddd574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489b0e49949d4dc697427c58d684d2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba952c8e6b7f4c6eb886006d36b15bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17948f0e9cbd45d6b3e7c0c6ef192756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5b6aa82caaf4c2fbfab336171ccd29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f361658c6786488c93a91fa62be002d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}